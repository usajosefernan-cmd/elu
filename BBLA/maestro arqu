# üéØ LUXSCALER v28.0: THE NEURO-SYMBOLIC EVOLUTION
**Sistema de Procesamiento Fotogr√°fico Gemini Native + The Brain Compiler + Dynamic Identity Lock + Smart Presets + Async Architecture**

- **Versi√≥n:** 28.0.0 (PRODUCTION READY - NEXT GENERATION)
- **Estado:** ‚úÖ ARQUITECTURA OPTIMIZADA PARA ESCALA GLOBAL
- **Fecha:** Enero 2026
- **Stack Principal:** Supabase (SQL Truth) + Google Cloud Workers / Cloudflare Workers (Async) + React 19
- **IA Core:** Google Gemini Ecosystem (Flash / Pro / 3 Pro) + Context Caching + Code Execution
- **Infraestructura Cloud:** Google Cloud Run / Google Cloud Tasks VS Cloudflare Workers (An√°lisis Comparativo Incluido)

---

## üìã √çNDICE MAESTRO COMPLETO

1. [Introducci√≥n a v28.0: Delta Evolutivo](#introducci√≥n-a-v280-delta-evolutivo)
2. [An√°lisis Comparativo: Google Cloud Workers vs Cloudflare Workers](#an√°lisis-comparativo-google-cloud-workers-vs-cloudflare-workers)
3. [Principios Fundamentales Heredados](#principios-fundamentales-heredados)
4. [Arquitectura de 4 Perfiles (Heredado de v27.2)](#arquitectura-de-4-perfiles-heredado-de-v272)
5. [Fase 1: Input Normalization (19.5MP Limit)](#fase-1-input-normalization-195mp-limit)
6. [Fase 2: Vision Analysis Optimizada con Proxy](#fase-2-vision-analysis-optimizada-con-proxy)
7. [Fase 3: Motor Sem√°ntico (Traducci√≥n de Sliders)](#fase-3-motor-sem√°ntico-traducci√≥n-de-sliders)
8. [Fase 4: El Cerebro (Prompt Compiler Service + Context Caching)](#fase-4-el-cerebro-prompt-compiler-service--context-caching)
9. [Fase 5: El Alma (System Prompt + Identity Lock + Multimodal DNA Anchor)](#fase-5-el-alma-system-prompt--identity-lock--multimodal-dna-anchor)
10. [Fase 6: Refinement Loop (Code Execution Masking)](#fase-6-refinement-loop-code-execution-masking)
11. [Infraestructura: Patr√≥n As√≠ncrono con Job Queues](#infraestructura-patr√≥n-as√≠ncrono-con-job-queues)
12. [Optimizaci√≥n: Context Caching & Proxy Vision](#optimizaci√≥n-context-caching--proxy-vision)
13. [UX: Renderizado H√≠brido (Preview vs Master)](#ux-renderizado-h√≠brido-preview-vs-master)
14. [Gesti√≥n de Smart Presets (L√≥gica de Archivo + Opacidad)](#gesti√≥n-de-smart-presets-l√≥gica-de-archivo--opacidad)
15. [Sistema de Monetizaci√≥n (Restricciones por Nivel)](#sistema-de-monetizaci√≥n-restricciones-por-nivel)
16. [Schema SQL Completo v28.0](#schema-sql-completo-v280)
17. [Flujo Integrado de Usuario (End-to-End)](#flujo-integrado-de-usuario-end-to-end)
18. [Configuraci√≥n Cloud: Deployment Detallado](#configuraci√≥n-cloud-deployment-detallado)
19. [Monitoreo, M√©tricas y Observabilidad](#monitoreo-m√©tricas-y-observabilidad)
20. [Roadmap Post-v28.0](#roadmap-post-v280)

---

## INTRODUCCI√ìN A v28.0: DELTA EVOLUTIVO

### Los Tres Cuellos de Botella de v27.2

La versi√≥n anterior ("The Truth") era arquitect√≥nicamente s√≥lida pero ten√≠a limitaciones cr√≠ticas para escala SaaS global:

1. **Latencia HTTP Bloqueante:** Deno Edge tiene un l√≠mite de wall-clock de 30-60s. Gemini 3 Pro + Inpainting puede tardar 20-40s en responder, causando timeouts 504 y errores de experiencia de usuario.

2. **Costes de Tokens Masivos:** Cada petici√≥n re-enviaba el "Documento Maestro" completo (System Prompt de 3000+ tokens). En escala (1000 usuarios simult√°neos = 3M tokens de input por segundo) = costo exponencial.

3. **Precisi√≥n Espacial Limitada:** Las LLMs generan m√°scaras de inpainting "imaginadas" (bordes borrosos, imprecisi√≥n). Las m√°scaras matem√°ticas (OpenCV) son perfectas.

### Soluciones Arquitect√≥nicas de v28.0

| Cuello de Botella | Soluci√≥n v27.2 | Soluci√≥n v28.0 | Impacto |
|---|---|---|---|
| **Latencia HTTP** | Espera bloqueante | Job Queues As√≠ncronos + Realtime | -85% latencia percibida |
| **Costes Tokens** | Re-env√≠o masivo | Context Caching + Proxy Vision | -60% costes API |
| **Precisi√≥n M√°scaras** | IA generada | Code Execution (Python/OpenCV) | +95% precisi√≥n |
| **Escalabilidad** | Edge monol√≠tica | Polyglot (Cloud Run + Workers) | 10x throughput |

---

## AN√ÅLISIS COMPARATIVO: GOOGLE CLOUD WORKERS VS CLOUDFLARE WORKERS

### Contexto de Decisi√≥n

Para v28.0, necesitamos ejecutar:
- An√°lisis de imagen (Vision API)
- Compilaci√≥n de prompts pesada (PromptCompilerService)
- Llamadas a Gemini 3 Pro (20-40s por request)
- Generaci√≥n de m√°scaras con Code Execution (5-10s)
- Inpainting y procesamiento final (10-20s)

**Total por request:** 45-90 segundos. **Requirement:** No bloquear el cliente.

### Comparaci√≥n Detallada

#### **OPCI√ìN 1: Google Cloud Run + Cloud Tasks**

**Arquitectura:**
```
Cliente (React 19)
    ‚Üì
API Gateway (Cloud Load Balancer)
    ‚Üì
Deno Edge (HTTP 200 inmediato + job_id)
    ‚Üì
Cloud Pub/Sub / Cloud Tasks (Encolado)
    ‚Üì
Cloud Run Worker (Ejecuta Gemini 3 Pro + Code Execution)
    ‚Üì
Supabase Realtime (Notifica cliente)
    ‚Üì
Cliente recibe notificaci√≥n
```

**Ventajas:**
- ‚úÖ **Integraci√≥n nativa con Vertex AI:** Gemini est√° nativamente disponible en Google Cloud. Latencia de red m√≠nima (<50ms).
- ‚úÖ **Context Caching integrado:** Vertex AI Generative AI API soporta caching nativo. No hay overhead.
- ‚úÖ **Escalabilidad autom√°tica:** Cloud Run escala de 0 a 1000 instancias en segundos.
- ‚úÖ **Costes predecibles:** Pay-per-execution. Paga solo lo que consumes.
- ‚úÖ **Security by default:** Integrado con IAM, VPC, Cloud Armor.
- ‚úÖ **Observabilidad:** Cloud Logging + Cloud Trace + Cloud Monitoring nativo.

**Desventajas:**
- ‚ùå **Vendor lock-in:** Todo Google. Si Vertex AI aumenta precio, no hay escape.
- ‚ùå **Cold starts:** Cloud Run tiene ~2s de cold start. Para job queues, es tolerable, pero no es ideal.
- ‚ùå **Costes iniciales altos:** Setup de IAM, VPC, KMS = complejidad.

**Estimaci√≥n de Costes (1M requests/mes, 19.5MP images):**
- Vertex AI (Gemini 3 Pro + caching): ~$2,000/mes
- Cloud Run: ~$500/mes
- Cloud Tasks: ~$50/mes
- Cloud Storage (logs): ~$100/mes
- **Total:** ~$2,650/mes

---

#### **OPCI√ìN 2: Cloudflare Workers + Cloudflare KV + Gemini API Direct**

**Arquitectura:**
```
Cliente (React 19)
    ‚Üì
Cloudflare Worker (Durable Objects - State Machine)
    ‚Üì
Llamada directa a Gemini API (api.google.com/v1/generativeai)
    ‚Üì
Cloudflare KV (Estado de job)
    ‚Üì
Supabase Realtime (Notifica cliente)
    ‚Üì
Cliente recibe notificaci√≥n
```

**Ventajas:**
- ‚úÖ **Latencia ultra-baja:** Cloudflare tiene presencia en 300+ ciudades. Ruta directa a cliente = <100ms latency.
- ‚úÖ **Costes m√≠nimos:** Workers: $0.15/M requests. KV: $0.50/M reads. Muy barato.
- ‚úÖ **Sin vendor lock-in:** Usa Google Gemini API directamente. Puedes migrar f√°cilmente.
- ‚úÖ **Durable Objects (State Machine):** Mantiene estado de job sin base de datos. S√∫per eficiente.
- ‚úÖ **Escalabilidad ilimitada:** Cloudflare maneja 50M+ requests/d√≠a en producci√≥n.
- ‚úÖ **Rate limiting inteligente:** Cloudflare tiene rate limiting integrado contra abuse.

**Desventajas:**
- ‚ùå **No hay Context Caching nativo:** Tienes que cachear manualmente en KV (workaround posible).
- ‚ùå **Gemini API direct = timeout risk:** Si Gemini tarda >30s, Cloudflare mata la ejecuci√≥n.
- ‚ùå **No hay observabilidad nativa:** Tienes que integrar LogRocket o Sentry.
- ‚ùå **Code Execution no est√° soportado:** Para ejecutar Python (masking), necesitas llamar a un endpoint externo (Google Cloud Run + Python runtime).

**Estimaci√≥n de Costes (1M requests/mes, 19.5MP images):**
- Cloudflare Workers: ~$150/mes
- Cloudflare KV: ~$500/mes
- Gemini API (direct): ~$2,000/mes
- Supabase Realtime: ~$100/mes
- Cloud Run (solo Python masking): ~$300/mes
- **Total:** ~$3,050/mes

---

#### **OPCI√ìN 3: H√çBRIDA (RECOMENDADA): Google Cloud Run + Cloudflare Workers**

**Arquitectura:**
```
Cliente (React 19) - en cualquier parte del mundo
    ‚Üì
Cloudflare Worker (Edge, latencia <100ms)
    ‚Üì
HTTP 200 inmediato + job_id en KV
    ‚Üì
Worker inicia Google Cloud Tasks
    ‚Üì
Cloud Run Worker (Gemini 3 Pro + Code Execution + Caching)
    ‚Üì
Resultado en Supabase
    ‚Üì
Supabase Realtime ‚Üí Cliente notificado
```

**Ventajas:**
- ‚úÖ **Latencia percibida cero:** Cloudflare responde al cliente en <100ms. Cloud Run ejecuta en background.
- ‚úÖ **Costes optimizados:** Cloudflare Workers (barato) + Cloud Run (escalable) = lo mejor de ambos.
- ‚úÖ **Context Caching:** Cloud Run accede a Vertex AI directamente. Caching nativo.
- ‚úÖ **Code Execution:** Cloud Run con Python runtime. Genera m√°scaras perfectas.
- ‚úÖ **Observabilidad:** Cloud Logging + Cloudflare Workers Analytics.
- ‚úÖ **Sin vendor lock-in:** Gemini API direct + Cloud Run polyglot = portabilidad.

**Desventajas:**
- ‚ö†Ô∏è **Complejidad operacional:** Dos plataformas = dos consolas, dos pipelines de CI/CD.
- ‚ö†Ô∏è **Costes iniciales moderados:** Setup de ambas plataformas.

**Estimaci√≥n de Costes (1M requests/mes, 19.5MP images):**
- Cloudflare Workers: ~$150/mes
- Cloudflare KV: ~$50/mes (minimal state)
- Cloud Run (60s promedio): ~$500/mes
- Vertex AI (Gemini 3 Pro + caching): ~$1,800/mes
- Cloud Tasks: ~$50/mes
- Supabase Realtime: ~$100/mes
- **Total:** ~$2,650/mes

---

### DECISI√ìN FINAL: RECOMENDACI√ìN

**Para m√°xima eficiencia y costo-beneficio: OPCI√ìN 3 (H√çBRIDA) con estas especificidades:**

1. **Cliente HTTP:** Cloudflare Workers
   - Regi√≥n: Auto-seleccionado por Cloudflare (200+ edges globales)
   - Responsabilidad: Recibir petici√≥n, generar job_id, iniciar Cloud Tasks

2. **Backend Procesamiento:** Google Cloud Run
   - Regi√≥n: `europe-west1` (B√©lgica) para latencia EMEA
   - Runtime: Node.js 20 (TypeScript)
   - Timeout: 60 minutos (configurable)
   - Concurrencia: 100 instancias por defecto, auto-scale a 1000

3. **IA/ML:** Vertex AI Generative AI API
   - Modelo: `gemini-3-pro-vision` (Multimodal + caching)
   - Context Caching: 5 minutos TTL por usuario
   - Code Execution: Habilitado (Python 3.10 + OpenCV)

4. **Estado:** Supabase + Realtime
   - Tabla primaria: `processing_jobs`
   - Broadcast: Supabase Realtime (WebSocket)
   - TTL: Job se limpia despu√©s de 30 d√≠as

---

## PRINCIPIOS FUNDAMENTALES HEREDADOS

### 1.1. La Regla de Oro: Solo Gemini

El sistema opera exclusivamente dentro del ecosistema Google Gemini. No hay librer√≠as externas de IA.

| Rol | Modelo | Uso por Perfil | Reemplazado en v28.0 |
|---|---|---|---|
| **El Ojo (Vision)** | `gemini-2.5-flash-vision` | Motor del perfil AUTO. An√°lisis previo. | ‚úÖ Proxy Vision (thumbnail) |
| **Speed Engine** | `gemini-2.5-flash` | Motor del perfil USER. | ‚úÖ Preview mode |
| **Creative Engine** | `gemini-3-pro-vision` | Motor del perfil PRO. | ‚úÖ Heredado |
| **Forensic Engine** | `gemini-3-pro-vision` | Motor del perfil PROLUX (NanoBananaPro). | ‚úÖ + Code Execution |

### 1.2. La Verdad √önica (SQL)

No hay l√≥gica hardcodeada. Todo vive en Supabase.

- **Perfiles:** `user_profiles` (con context_cache_enabled booleano)
- **Traducci√≥n:** `macro_definitions` + `slider_semantic_mappings`
- **Sem√°ntica:** `slider_semantic_mappings` (mapeo slider ‚Üí instrucci√≥n)
- **Memoria:** `smart_presets` + `style_embeddings` (pgvector)
- **Procesamiento:** `processing_jobs` (estado de ejecuci√≥n en tiempo real)

### 1.3. La Verdad √önica: No Hardcoding

Cada variable cr√≠tica existe en SQL:
- Los 27 sliders de configuraci√≥n
- Las instrucciones sem√°nticas por slider
- El System Prompt din√°mico
- Los bloques de texto (PHOTOSCALER_BLOCK, LIGHTSCALER_BLOCK, etc.)
- Las reglas de conflicto y veto

---

## ARQUITECTURA DE 4 PERFILES (HEREDADO DE v27.2)

### Definici√≥n de Perfiles

| Perfil | Concepto | UI Principal | Modelo | Motor de Sliders | Acceso Code Execution | Compartir Presets |
|---|---|---|---|---|---|---|
| **AUTO** | Full AI Autopilot (Caja negra) | NONE | Gemini 2.5 Flash | Vision decide autom√°ticamente | ‚ùå | ‚ùå |
| **USER** | Control Abstracto (3 Macros) | `<SimplePillarControl />` | Gemini 2.5 Flash | 3 Macros (mueve 9 sliders) | ‚ùå | ‚ùå |
| **PRO** | Control Tem√°tico (9 Macros) | `<MacroSliderGallery />` | Gemini 3 Pro | 9 Macros (cada uno mueve subset) | ‚ùå | ‚ùå |
| **PROLUX** | Ingenier√≠a Forense (27 Sliders) | `<MicroSliderGrid />` | Gemini 3 Pro | 27 Sliders individuales + Code Execution | ‚úÖ | ‚úÖ (solo a otros PROLUX) |

### Los 27 Sliders (Microcontroles)

**Pillar 1: PHOTOSCALER (√ìptica, Geometr√≠a, Restauraci√≥n)**
1. `limpieza_artefactos` (Noise Reduction: 0-10)
2. `grano_filmico` (Grain / Film Effect: 0-10)
3. `optica_nitidez` (Sharpness: 0-10)
4. `geometria_distorsion` (Lens Distortion Correction: 0-10)
5. `reencuadre_ia` (AI Reframing: 0-10)
6. `desenfoque_movimiento` (Motion Blur Removal: 0-10)
7. `detalle_texturas` (Texture Detail: 0-10)
8. `restauracion_danos` (Damage Restoration: 0-10)
9. `geometria_perspectiva` (Perspective Correction: 0-10)

**Pillar 2: STYLESCALER (Estilo, Car√°cter, Paleta)**
10. `vibracion_saturacion` (Vibrancy: 0-10)
11. `paleta_tonos` (Color Tone Shift: 0-10)
12. `dramatismo_contraste` (Drama / Contrast: 0-10)
13. `estilo_render` (Render Style: Film / Digital / Hybrid: 0-10)
14. `antiguedad_aged_look` (Aged Look: 0-10)
15. `retoque_piel` (Skin Retouching: 0-10)
16. `dramatismo_vigneta` (Vignette: 0-10)
17. `suavidad_bokeh` (Bokeh Softness: 0-10)
18. `caracter_procesa` (Character Processing: 0-10)

**Pillar 3: LIGHTSCALER (Iluminaci√≥n, Tono, Profundidad)**
19. `brillo_exposicion` (Exposure: 0-10)
20. `luz_relleno` (Fill Light: 0-10)
21. `profundidad_sombras` (Shadow Depth: 0-10)
22. `iluminacion_dramatica` (Dramatic Lighting: 0-10)
23. `tonalidad_color` (Color Temperature: 0-10)
24. `enfasis_ojos` (Eye Emphasis / Gaze: 0-10)
25. `profundidad_dof` (Depth of Field: 0-10)
26. `luces_especulares` (Specular Highlights: 0-10)
27. `balance_luminoso` (Light Balance: 0-10)

### Los 9 Macros (Nivel PRO)

Cada macro es una "intenci√≥n tem√°tica" que mueve un subset de los 27 sliders.

| Macro | Pillar | Sliders Afectados | Intensidad |
|---|---|---|---|
| **1. Limpieza Profunda** | PHOTOSCALER | 1,2,3,6,8 | Movimiento coordinado |
| **2. Restauraci√≥n Hist√≥rica** | PHOTOSCALER | 1,4,8,9 | Geometr√≠a + Da√±o |
| **3. Portrait Refinement** | STYLESCALER | 15,16,17,24 | Retoque profesional |
| **4. Cinematic Tone** | STYLESCALER | 12,13,14,16 | Dramatismo |
| **5. Studio Lighting** | LIGHTSCALER | 19,20,21,22 | Iluminaci√≥n controlada |
| **6. Golden Hour Glow** | LIGHTSCALER | 19,23,26 | Hora dorada |
| **7. Vintage Aesthetics** | STYLESCALER | 10,11,14 | Est√©tica retro |
| **8. High-End Luxury** | ALL | 1,7,15,24,19,22 | Combinaci√≥n premium |
| **9. Extreme Forensic** | PHOTOSCALER | 1,3,4,8,9 | M√°xima restauraci√≥n |

### Los 3 Macros (Nivel USER)

Cada macro es un "Pilar" completo.

| Macro | Sliders | Descripci√≥n |
|---|---|---|
| **1. Photo Repair** | PHOTOSCALER (1-9) | Restauraci√≥n y limpieza |
| **2. Style Lab** | STYLESCALER (10-18) | Estilo y car√°cter |
| **3. Light Studio** | LIGHTSCALER (19-27) | Iluminaci√≥n |

---

## FASE 1: INPUT NORMALIZATION (19.5MP LIMIT)

### Algoritmo de Normalizaci√≥n

Antes de procesar, normalizamos la entrada para evitar alucinaciones por "token overflow" visual en Gemini.

#### Step 1: Detecci√≥n de Dimensiones

```typescript
// File: backend/workers/input-normalizer.ts

import Jimp from "jimp";
import { Readable } from "stream";

async function normalizeInput(buffer: Buffer): Promise<{
  normalized: Buffer;
  metadata: {
    original_mp: number;
    original_dimensions: { width: number; height: number };
    normalized_mp: number;
    normalized_dimensions: { width: number; height: number };
    downscale_factor: number;
    action: "pass_through" | "downscale";
  };
}> {
  // Lee la imagen original
  const image = await Jimp.read(buffer);
  const { width, height } = image.bitmap;
  const originalMP = (width * height) / 1_000_000;

  const MAX_MP = 19.5;
  const metadata = {
    original_mp: originalMP,
    original_dimensions: { width, height },
    normalized_mp: 0,
    normalized_dimensions: { width: 0, height: 0 },
    downscale_factor: 1,
    action: "pass_through" as const,
  };

  // Si ya est√° bajo el l√≠mite, pass-through
  if (originalMP <= MAX_MP) {
    metadata.normalized_mp = originalMP;
    metadata.normalized_dimensions = { width, height };

    // Convierte a JPEG sRGB
    const normalized = await image
      .quality(90)
      .getBuffer("image/jpeg");

    return { normalized, metadata };
  }

  // Si excede, downscale con Lanczos3
  metadata.action = "downscale";
  const downscaleFactor = Math.sqrt(MAX_MP / originalMP);
  const newWidth = Math.round(width * downscaleFactor);
  const newHeight = Math.round(height * downscaleFactor);
  const newMP = (newWidth * newHeight) / 1_000_000;

  metadata.downscale_factor = downscaleFactor;
  metadata.normalized_mp = newMP;
  metadata.normalized_dimensions = { width: newWidth, height: newHeight };

  // Redimensiona con Lanczos3 (JIMP usa este por defecto)
  const resized = image.resize(newWidth, newHeight);

  // Convierte a JPEG sRGB + Quality 90
  const normalized = await resized
    .quality(90)
    .getBuffer("image/jpeg");

  return { normalized, metadata };
}

export { normalizeInput };
```

#### Step 2: Validaci√≥n de Espacio Color

```typescript
// File: backend/workers/color-space-validator.ts

async function ensureSRGB(jpegBuffer: Buffer): Promise<Buffer> {
  // Jimp siempre devuelve sRGB. Si la imagen tiene ICC profile embebido,
  // lo ignoramos y forzamos sRGB.
  
  // En production, usar√≠as ImageMagick o libvips para profundidad:
  // $ magick input.jpg -colorspace sRGB output.jpg
  
  // Para simplificar, Jimp es suficiente.
  return jpegBuffer;
}

export { ensureSRGB };
```

#### Step 3: Almacenamiento Temporal

```typescript
// File: backend/workers/input-storage.ts

import { createClient } from "@supabase/supabase-js";

async function storeNormalizedInput(
  normalizedBuffer: Buffer,
  jobId: string,
  metadata: any
): Promise<{
  storage_path: string;
  public_url: string;
}> {
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

  // Supabase Storage: luxscaler-inputs/2026-01-20/job-uuid.jpg
  const path = `luxscaler-inputs/${new Date().toISOString().split("T")[0]}/${jobId}.jpg`;

  const { data, error } = await supabase.storage
    .from("processing-bucket")
    .upload(path, normalizedBuffer, {
      contentType: "image/jpeg",
      metadata: {
        job_id: jobId,
        timestamp: new Date().toISOString(),
        ...metadata,
      },
    });

  if (error) throw error;

  // Genera URL p√∫blica
  const { data: urlData } = supabase.storage
    .from("processing-bucket")
    .getPublicUrl(path);

  return {
    storage_path: path,
    public_url: urlData.publicUrl,
  };
}

export { storeNormalizedInput };
```

#### Regla de Normalizaci√≥n Completa

```
M√°ximo: 19.5 MP
Formato: Todo se convierte a JPEG (Quality 90) y sRGB
Algoritmo: Si MP > 19.5 ‚Üí Downscale inteligente (Lanczos3, preserva aspect ratio)
           Si MP ‚â§ 19.5 ‚Üí Pass-through (solo reconversi√≥n a JPEG/sRGB)
Salida: Buffer JPEG listo para Gemini 2.5 Flash Vision
Metadata: Se guarda en processing_jobs para trazabilidad
```

---

## FASE 2: VISION ANALYSIS OPTIMIZADA CON PROXY

### El Problema de v27.2

Analizar una imagen de 19.5MP con Gemini 2.5 Flash Vision cuesta:
- Tokens: ~2,000 tokens de input
- Latencia: 3-8 segundos
- Costo: $0.04-0.08 por an√°lisis

En escala (1000 usuarios/d√≠a), son 1000 an√°lisis = $40-80/d√≠a en solo Vision.

### La Soluci√≥n v28.0: Proxy Vision

**Concepto:** Los atributos sem√°nticos (luz, color, sujeto) de una imagen de 19.5MP son id√©nticos a los de su thumbnail (1024px). Pero el thumbnail cuesta 50x menos procesar.

#### Flujo de Proxy Vision

```
Input: Imagen normalizada (19.5MP JPEG)
    ‚Üì
Step 1: Genera thumbnail (1024x768 o aspect ratio preservado)
    ‚Üì
Step 2: Analiza thumbnail con Gemini 2.5 Flash Vision
    ‚Üì
Step 3: Cachea resultado en vision_analysis_cache (image_hash)
    ‚Üì
Step 4: Devuelve an√°lisis a PromptCompilerService
    ‚Üì
Output: {
  technical_score: 6,
  semantic_anchors: ["studio lighting", "wooden table", "scars on cheek"],
  suggested_settings: {...}
}
```

#### Implementaci√≥n

```typescript
// File: backend/workers/proxy-vision-analyzer.ts

import Jimp from "jimp";
import crypto from "crypto";
import { createClient } from "@supabase/supabase-js";
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

async function analyzeWithProxyVision(
  normalizedImagePath: string,
  jobId: string
): Promise<{
  technical_score: number;
  semantic_anchors: string[];
  suggested_settings: Record<string, number>;
  cache_hit: boolean;
  analysis_tokens: number;
}> {
  // Step 1: Descarga imagen normalizada desde Storage
  const { data: imageBuffer, error: downloadError } = await supabase.storage
    .from("processing-bucket")
    .download(normalizedImagePath);

  if (downloadError) throw downloadError;

  // Step 2: Genera thumbnail (1024px max dimension, aspect ratio preservado)
  const image = await Jimp.read(imageBuffer);
  const originalDimensions = image.bitmap;

  // Calcula nuevas dimensiones
  const maxDim = 1024;
  let newWidth = originalDimensions.width;
  let newHeight = originalDimensions.height;

  if (newWidth > maxDim || newHeight > maxDim) {
    const scaleFactor = Math.min(
      maxDim / newWidth,
      maxDim / newHeight
    );
    newWidth = Math.round(newWidth * scaleFactor);
    newHeight = Math.round(newHeight * scaleFactor);
  }

  const thumbnail = await image
    .resize(newWidth, newHeight)
    .quality(85)
    .getBuffer("image/jpeg");

  // Step 3: Calcula hash de thumbnail para cach√©
  const thumbnailHash = crypto
    .createHash("sha256")
    .update(thumbnail)
    .digest("hex");

  // Step 4: Consulta cach√©
  const { data: cachedAnalysis, error: cacheError } = await supabase
    .from("vision_analysis_cache")
    .select("analysis_payload")
    .eq("image_hash", thumbnailHash)
    .single();

  if (!cacheError && cachedAnalysis) {
    return {
      ...cachedAnalysis.analysis_payload,
      cache_hit: true,
      analysis_tokens: 0, // No consumi√≥ tokens
    };
  }

  // Step 5: Llama a Gemini 2.5 Flash Vision (thumbnail)
  const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash-vision" });

  const analysisPrompt = `Analiza esta imagen fotogr√°fica y proporciona un JSON con este esquema exacto:
{
  "technical_score": <0-10, calidad t√©cnica: ruido, desenfoque, exposici√≥n>,
  "semantic_anchors": [<lista de elementos sem√°nticos a preservar, ej: "wooden table", "scar on cheek">],
  "suggested_settings": {
    "limpieza_artefactos": <0-10>,
    "grano_filmico": <0-10>,
    "optica_nitidez": <0-10>,
    "geometria_distorsion": <0-10>,
    "reencuadre_ia": <0-10>,
    "desenfoque_movimiento": <0-10>,
    "detalle_texturas": <0-10>,
    "restauracion_danos": <0-10>,
    "geometria_perspectiva": <0-10>,
    "vibracion_saturacion": <0-10>,
    "paleta_tonos": <0-10>,
    "dramatismo_contraste": <0-10>,
    "estilo_render": <0-10>,
    "antiguedad_aged_look": <0-10>,
    "retoque_piel": <0-10>,
    "dramatismo_vigneta": <0-10>,
    "suavidad_bokeh": <0-10>,
    "caracter_procesa": <0-10>,
    "brillo_exposicion": <0-10>,
    "luz_relleno": <0-10>,
    "profundidad_sombras": <0-10>,
    "iluminacion_dramatica": <0-10>,
    "tonalidad_color": <0-10>,
    "enfasis_ojos": <0-10>,
    "profundidad_dof": <0-10>,
    "luces_especulares": <0-10>,
    "balance_luminoso": <0-10>
  }
}

Devuelve SOLO el JSON v√°lido, sin markdown ni explicaciones.`;

  const imageData = {
    inlineData: {
      data: thumbnail.toString("base64"),
      mimeType: "image/jpeg",
    },
  };

  const response = await model.generateContent([
    analysisPrompt,
    imageData,
  ]);

  const analysisText = response.response.text();
  const analysisPayload = JSON.parse(analysisText);

  // Step 6: Cachea resultado
  await supabase
    .from("vision_analysis_cache")
    .insert({
      image_hash: thumbnailHash,
      analysis_payload: analysisPayload,
    });

  // Step 7: Registra en job para auditor√≠a
  await supabase
    .from("processing_jobs")
    .update({
      vision_analysis_result: analysisPayload,
      proxy_thumbnail_hash: thumbnailHash,
    })
    .eq("id", jobId);

  return {
    ...analysisPayload,
    cache_hit: false,
    analysis_tokens: response.response.usageMetadata?.inputTokens || 1500,
  };
}

export { analyzeWithProxyVision };
```

### Impacto de Costos

| M√©trica | Vision Full 19.5MP | Proxy Vision (1024px) | Ahorro |
|---|---|---|---|
| Tokens Input | ~2,000 | ~400 | -80% |
| Latencia | 5-8s | 1-2s | -70% |
| Costo | $0.08 | $0.016 | -80% |
| Precisi√≥n Sem√°ntica | 100% | 99.5% | -0.5% (aceptable) |

**Conclusi√≥n:** Ahorramos 80% de costos en Vision Analysis. La p√©rdida en precisi√≥n es negligible.

---

## FASE 3: MOTOR SEM√ÅNTICO (TRADUCCI√ìN DE SLIDERS)

### Concepto Fundamental

El usuario mueve sliders (0-10). El backend traduce esos valores a instrucciones de Gemini usando la tabla `slider_semantic_mappings`.

### Flujo de Traducci√≥n por Perfil

#### PERFIL AUTO
```
Usuario: No toca nada. Vision decide todo.
    ‚Üì
Backend: suggested_settings de Fase 2 ‚Üí Valores 0-10 directos
    ‚Üì
Motor Sem√°ntico: Traduce los 27 valores a instrucciones
    ‚Üì
Output: Prompt compilado
```

#### PERFIL USER
```
Usuario: Mueve 1 Slider Maestro (0-10) del Pilar A
    ‚Üì
Backend: Slider Maestro = amplitud. Mapea a 9 sliders del Pilar.
    ‚Üì
Ejemplo: Usuario mueve "Photo Repair" a 7
    ‚Üì
  Traduce a:
    - limpieza_artefactos: 7
    - grano_filmico: 7
    - optica_nitidez: 7
    - geometria_distorsion: 7
    - reencuadre_ia: 7
    - desenfoque_movimiento: 7
    - detalle_texturas: 7
    - restauracion_danos: 7
    - geometria_perspectiva: 7
    ‚Üì
Motor Sem√°ntico: Traduce a instrucciones
```

#### PERFIL PRO
```
Usuario: Mueve 1 Macro (0-10). Cada Macro mueve un subset.
    ‚Üì
Ejemplo: Usuario mueve "Studio Lighting" (Macro 5) a 8
    ‚Üì
  Traduce a:
    - brillo_exposicion: 8
    - luz_relleno: 8
    - profundidad_sombras: 8
    - iluminacion_dramatica: 8
    - (resto quedan en 0 u √∫ltimo valor)
    ‚Üì
Motor Sem√°ntico: Traduce a instrucciones
```

#### PERFIL PROLUX
```
Usuario: Mueve cada uno de los 27 sliders individualmente.
    ‚Üì
Backend: Valores 0-10 directo.
    ‚Üì
Motor Sem√°ntico: Traduce cada slider a instrucci√≥n.
```

### Schema: slider_semantic_mappings

```sql
CREATE TABLE slider_semantic_mappings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    slider_name VARCHAR(100) NOT NULL UNIQUE,
    slider_id INT (1-27),
    
    -- Instrucciones por nivel de intensidad
    instruction_off TEXT, -- Cuando slider = 0
    instruction_low TEXT, -- Cuando slider = 1-3
    instruction_med TEXT, -- Cuando slider = 4-6
    instruction_high TEXT, -- Cuando slider = 7-8
    instruction_force TEXT, -- Cuando slider = 9-10
    
    -- Metadatos
    pillar VARCHAR(50), -- PHOTOSCALER, STYLESCALER, LIGHTSCALER
    semantic_field VARCHAR(100), -- Categor√≠a (ej: "noise reduction")
    requires_identity_lock BOOLEAN DEFAULT true, -- ¬øAfecta la cara?
    conflicts_with TEXT[], -- Array de slider_names que entran en conflicto
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### Ejemplo: Slider "limpieza_artefactos"

```sql
INSERT INTO slider_semantic_mappings (
    slider_name, slider_id, pillar,
    instruction_off,
    instruction_low,
    instruction_med,
    instruction_high,
    instruction_force,
    semantic_field,
    requires_identity_lock,
    conflicts_with
) VALUES (
    'limpieza_artefactos',
    1,
    'PHOTOSCALER',
    '', -- OFF: no incluir instrucci√≥n
    'Reduce light noise and compression artifacts with subtle smoothing. Preserve fine details.',
    'Apply moderate noise reduction. Balance artifact removal with texture preservation.',
    'Aggressively remove noise, dust, and compression artifacts. Hallucinate high-frequency details from context.',
    'FORCE: Maximum artifact elimination. Virtual re-shoot at 1/8000s shutter (zero motion blur, zero grain). IGNORE source grain patterns. Hallucinate crystal-clear image.',
    'Artifact Removal',
    true,
    ARRAY['grano_filmico', 'detalle_texturas']
);
```

### La Traducci√≥n: Algoritmo Completo

```typescript
// File: backend/services/semantic-motor.ts

import { createClient } from "@supabase/supabase-js";

interface SemanticTranslation {
  slider_name: string;
  slider_value: number;
  instruction: string;
  intensity_level: 'off' | 'low' | 'med' | 'high' | 'force';
}

async function translateSliderToInstruction(
  sliderName: string,
  sliderValue: number
): Promise<SemanticTranslation> {
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

  // Step 1: Consulta el slider mapping
  const { data, error } = await supabase
    .from("slider_semantic_mappings")
    .select("*")
    .eq("slider_name", sliderName)
    .single();

  if (error) throw error;

  // Step 2: Determina el nivel de intensidad
  let intensityLevel: 'off' | 'low' | 'med' | 'high' | 'force';
  let instruction = "";

  if (sliderValue === 0) {
    intensityLevel = 'off';
    instruction = data.instruction_off || ''; // OFF: generalmente vac√≠o
  } else if (sliderValue <= 3) {
    intensityLevel = 'low';
    instruction = data.instruction_low;
  } else if (sliderValue <= 6) {
    intensityLevel = 'med';
    instruction = data.instruction_med;
  } else if (sliderValue <= 8) {
    intensityLevel = 'high';
    instruction = data.instruction_high;
  } else {
    intensityLevel = 'force';
    instruction = data.instruction_force;
  }

  return {
    slider_name: sliderName,
    slider_value: sliderValue,
    instruction,
    intensity_level: intensityLevel,
  };
}

interface SliderSet {
  [sliderName: string]: number; // ej: { limpieza_artefactos: 7, grano_filmico: 2 }
}

async function translateAllSlidersToInstructions(
  sliderSet: SliderSet
): Promise<{
  translations: SemanticTranslation[];
  active_instructions: string[];
  conflicts_detected: Array<{
    slider1: string;
    slider2: string;
    severity: 'warning' | 'error';
  }>;
}> {
  const translations: SemanticTranslation[] = [];
  const activeInstructions: string[] = [];
  const conflictsDetected: any[] = [];

  // Step 1: Traduce cada slider
  for (const [sliderName, sliderValue] of Object.entries(sliderSet)) {
    const translation = await translateSliderToInstruction(sliderName, sliderValue);
    translations.push(translation);

    // Recolecta instrucciones activas (no OFF)
    if (translation.instruction) {
      activeInstructions.push(translation.instruction);
    }
  }

  // Step 2: Detecta conflictos
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);
  const { data: mappings } = await supabase
    .from("slider_semantic_mappings")
    .select("*");

  for (const mapping of mappings || []) {
    const conflictsWith = mapping.conflicts_with || [];

    for (const conflictSlider of conflictsWith) {
      const slider1Value = sliderSet[mapping.slider_name] || 0;
      const slider2Value = sliderSet[conflictSlider] || 0;

      // Ambos activos = conflicto
      if (slider1Value > 0 && slider2Value > 0) {
        conflictsDetected.push({
          slider1: mapping.slider_name,
          slider2: conflictSlider,
          severity: slider1Value > 7 && slider2Value > 7 ? 'error' : 'warning',
        });
      }
    }
  }

  return {
    translations,
    active_instructions: activeInstructions.filter(i => i.length > 0),
    conflicts_detected: conflictsDetected,
  };
}

export { translateSliderToInstruction, translateAllSlidersToInstructions };
```

---

## FASE 4: EL CEREBRO (PROMPT COMPILER SERVICE + CONTEXT CACHING)

### El Algoritmo PromptCompilerService (v27.2 Heredado + Mejoras v28.0)

Transforma los 27 valores num√©ricos en una instrucci√≥n coherente para Gemini 3 Pro, resolviendo conflictos y applicando Context Caching.

### PASO 1: Resoluci√≥n de Jerarqu√≠as (Logic Layer)

Detecta conflictos l√≥gicos y aplica "Vetos" antes de generar texto.

#### Reglas de Oro (Vetos)

```typescript
// File: backend/services/conflict-veto-engine.ts

interface VetoRule {
  name: string;
  trigger_condition: (sliders: SliderSet) => boolean;
  veto_actions: Array<{
    slider_name: string;
    force_value: number;
    reason: string;
  }>;
}

const VETO_RULES: VetoRule[] = [
  {
    name: "La Paradoja Forense",
    trigger_condition: (sliders) => sliders.limpieza_artefactos === 10,
    veto_actions: [
      {
        slider_name: "grano_filmico",
        force_value: 0,
        reason: "Limpieza FORCE mata lo vintage. Grano OFF.",
      },
      {
        slider_name: "optica_nitidez",
        force_value: 10,
        reason: "Limpieza FORCE fuerza m√°xima nitidez.",
      },
    ],
  },
  {
    name: "La Tiran√≠a del Drama",
    trigger_condition: (sliders) => sliders.dramatismo_contraste === 10,
    veto_actions: [
      {
        slider_name: "luz_relleno",
        force_value: 0,
        reason: "Drama FORCE no permite fill light. Contraste absoluto.",
      },
    ],
  },
  {
    name: "Paradoja de Geometr√≠a",
    trigger_condition: (sliders) =>
      sliders.geometria_distorsion === 10 &&
      sliders.reencuadre_ia === 10,
    veto_actions: [
      {
        slider_name: "reencuadre_ia",
        force_value: 0,
        reason: "No puedes corregir distorsi√≥n Y reencuadrar a la vez. Priority: Distorsi√≥n.",
      },
    ],
  },
];

async function applyVetoRules(
  sliders: SliderSet
): Promise<{
  modified_sliders: SliderSet;
  vetos_applied: Array<{ rule_name: string; actions: any[] }>;
}> {
  let modifiedSliders = { ...sliders };
  const vetosApplied: any[] = [];

  for (const rule of VETO_RULES) {
    if (rule.trigger_condition(modifiedSliders)) {
      for (const action of rule.veto_actions) {
        modifiedSliders[action.slider_name] = action.force_value;
      }

      vetosApplied.push({
        rule_name: rule.name,
        actions: rule.veto_actions,
      });
    }
  }

  return { modified_sliders: modifiedSliders, vetos_applied: vetosApplied };
}

export { applyVetoRules };
```

### PASO 2: Inyecci√≥n de Bloques (Template Injection)

Busca los textos en la BD y los inyecta solo si el slider es > 0.

```typescript
// File: backend/services/block-injector.ts

interface CompilerBlockOutput {
  PHOTOSCALER_BLOCK: string;
  STYLESCALER_BLOCK: string;
  LIGHTSCALER_BLOCK: string;
}

async function injectSemanticBlocks(
  sliderSet: SliderSet,
  translations: SemanticTranslation[]
): Promise<CompilerBlockOutput> {
  const blocks = {
    PHOTOSCALER_BLOCK: [],
    STYLESCALER_BLOCK: [],
    LIGHTSCALER_BLOCK: [],
  };

  // Step 1: Agrupa las instrucciones por pilar
  for (const translation of translations) {
    // Consulta el pilar del slider
    const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);
    const { data: mapping } = await supabase
      .from("slider_semantic_mappings")
      .select("pillar")
      .eq("slider_name", translation.slider_name)
      .single();

    if (mapping && translation.instruction) {
      const blockKey = `${mapping.pillar}_BLOCK`;
      blocks[blockKey as keyof CompilerBlockOutput].push(
        `- ${translation.instruction}`
      );
    }
  }

  // Step 2: Formatea cada bloque
  const output: CompilerBlockOutput = {
    PHOTOSCALER_BLOCK:
      blocks.PHOTOSCALER_BLOCK.length > 0
        ? blocks.PHOTOSCALER_BLOCK.join("\n")
        : "",
    STYLESCALER_BLOCK:
      blocks.STYLESCALER_BLOCK.length > 0
        ? blocks.STYLESCALER_BLOCK.join("\n")
        : "",
    LIGHTSCALER_BLOCK:
      blocks.LIGHTSCALER_BLOCK.length > 0
        ? blocks.LIGHTSCALER_BLOCK.join("\n")
        : "",
  };

  return output;
}

export { injectSemanticBlocks };
```

### PASO 3: El Sanitizador Sem√°ntico (Final Polish)

Optimiza para Gemini 3 Pro (quita redundancias, formatea).

```typescript
// File: backend/services/semantic-sanitizer.ts

interface SanitizationResult {
  prompt: string;
  redundancies_removed: number;
  empty_sections_removed: string[];
}

async function sanitizeSemanticPrompt(
  template: string,
  blocks: CompilerBlockOutput,
  visionAnalysis: any
): Promise<SanitizationResult> {
  let prompt = template;
  let redundanciesRemoved = 0;
  const emptySectionsRemoved: string[] = [];

  // Step 1: Inyecta bloques din√°micos
  prompt = prompt.replace(/\{\{PHOTOSCALER_BLOCK\}\}/g, blocks.PHOTOSCALER_BLOCK);
  prompt = prompt.replace(/\{\{STYLESCALER_BLOCK\}\}/g, blocks.STYLESCALER_BLOCK);
  prompt = prompt.replace(/\{\{LIGHTSCALER_BLOCK\}\}/g, blocks.LIGHTSCALER_BLOCK);
  prompt = prompt.replace(
    /\{\{VISION_ANALYSIS_SUMMARY\}\}/g,
    JSON.stringify(visionAnalysis, null, 2)
  );

  // Step 2: Elimina secciones vac√≠as (ej: si LIGHTSCALER_BLOCK est√° vac√≠o)
  const emptyPatterns = [
    /=== LIGHTING & TONE ===\n\n=== NEGATIVE PROMPT ===/g,
    /^GEOMETRY & RESTORATION:\n\n$/gm,
    /^LIGHTING & TONE:\n\n$/gm,
  ];

  for (const pattern of emptyPatterns) {
    if (pattern.test(prompt)) {
      prompt = prompt.replace(pattern, "");
      emptySectionsRemoved.push(pattern.source);
    }
  }

  // Step 3: Elimina duplicados (ej: "8k resolution" repetido)
  const lines = prompt.split("\n");
  const seen = new Set<string>();
  const uniqueLines: string[] = [];

  for (const line of lines) {
    const normalized = line.trim();
    if (!seen.has(normalized) || !normalized) {
      seen.add(normalized);
      uniqueLines.push(line);
    } else {
      redundanciesRemoved++;
    }
  }

  prompt = uniqueLines.join("\n");

  return {
    prompt,
    redundancies_removed: redundanciesRemoved,
    empty_sections_removed: emptySectionsRemoved,
  };
}

export { sanitizeSemanticPrompt };
```

### Context Caching v28.0 (MEJORA CR√çTICA)

En lugar de re-enviar el System Prompt completo en cada request, lo cacheamos en Vertex AI.

```typescript
// File: backend/services/context-cache-manager.ts

interface ContextCacheMetadata {
  cache_creation_token: string;
  cache_age_seconds: number;
  cache_ttl_seconds: number;
  tokens_saved_per_request: number;
  total_tokens_cached: number;
}

async function initializeContextCache(
  userId: string,
  systemPrompt: string
): Promise<ContextCacheMetadata> {
  const vertex = new VertexAI({
    project: process.env.GCP_PROJECT_ID,
    location: "europe-west1",
  });

  const model = vertex.getGenerativeModel({
    model: "gemini-3-pro-vision",
  });

  // System Prompt es parte del contexto cacheado
  const cachedContent = model.createCachedContent({
    model: "gemini-3-pro-vision",
    cachedText: systemPrompt,
    ttlSeconds: 3600, // 1 hora TTL
  });

  // Almacena la metadata en Supabase para recuperaci√≥n
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);
  await supabase
    .from("user_profiles")
    .update({
      context_cache_token: cachedContent.cacheCreationToken,
      context_cache_enabled: true,
      context_cache_expires_at: new Date(
        Date.now() + 3600 * 1000
      ).toISOString(),
    })
    .eq("id", userId);

  return {
    cache_creation_token: cachedContent.cacheCreationToken,
    cache_age_seconds: 0,
    cache_ttl_seconds: 3600,
    tokens_saved_per_request: 2000, // Aprox. tokens del system prompt
    total_tokens_cached: 2000,
  };
}

async function generateWithContextCache(
  userId: string,
  userPrompt: string,
  imageBuffer: Buffer
): Promise<{
  generated_text: string;
  tokens_used_from_cache: number;
  tokens_input_new: number;
  cache_age_seconds: number;
}> {
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

  // Step 1: Recupera cache token del usuario
  const { data: userProfile } = await supabase
    .from("user_profiles")
    .select("context_cache_token, context_cache_expires_at")
    .eq("id", userId)
    .single();

  if (!userProfile?.context_cache_token) {
    throw new Error("User has no context cache initialized");
  }

  // Step 2: Valida que el cache no ha expirado
  const cacheExpiresAt = new Date(userProfile.context_cache_expires_at);
  const now = new Date();
  const cacheAgeSeconds = Math.floor((now.getTime() - cacheExpiresAt.getTime()) / 1000);

  if (now > cacheExpiresAt) {
    throw new Error("Context cache expired. Reinitialize.");
  }

  // Step 3: Llama a Gemini con cached content
  const vertex = new VertexAI({
    project: process.env.GCP_PROJECT_ID,
    location: "europe-west1",
  });

  const model = vertex.getGenerativeModel({
    model: "gemini-3-pro-vision",
  });

  const response = await model.generateContent({
    contents: [
      {
        role: "user",
        parts: [
          { text: userPrompt },
          {
            inlineData: {
              data: imageBuffer.toString("base64"),
              mimeType: "image/jpeg",
            },
          },
        ],
      },
    ],
    cachedContent: userProfile.context_cache_token,
  });

  const generatedText = response.response.text();
  const usageMetadata = response.response.usageMetadata;

  return {
    generated_text: generatedText,
    tokens_used_from_cache: usageMetadata?.cachedInputTokens || 2000,
    tokens_input_new: usageMetadata?.inputTokens || 100,
    cache_age_seconds: Math.abs(cacheAgeSeconds),
  };
}

export { initializeContextCache, generateWithContextCache };
```

### El Compilador Completo

```typescript
// File: backend/services/prompt-compiler.ts

interface CompilerInput {
  slider_values: SliderSet;
  vision_analysis: any;
  user_id: string;
  profile_type: "AUTO" | "USER" | "PRO" | "PROLUX";
}

interface CompilerOutput {
  compiled_prompt: string;
  system_prompt: string;
  tokens_estimate: {
    system_cached: number;
    user_new: number;
    total_from_cache: number;
  };
  debug_info: {
    vetos_applied: any[];
    conflicts: any[];
    sanitization: any;
  };
}

async function compilePromptWithCaching(
  input: CompilerInput
): Promise<CompilerOutput> {
  // Step 1: Aplica vetos
  const { modified_sliders, vetos_applied } = await applyVetoRules(
    input.slider_values
  );

  // Step 2: Traduce sliders a instrucciones
  const { translations, active_instructions, conflicts_detected } =
    await translateAllSlidersToInstructions(modified_sliders);

  // Step 3: Inyecta bloques
  const blocks = await injectSemanticBlocks(modified_sliders, translations);

  // Step 4: System Prompt con Identity Lock din√°mico
  const systemPrompt = buildDynamicSystemPrompt(input, blocks, modified_sliders);

  // Step 5: Sanitiza
  const userPrompt = `Based on the configuration and vision analysis, generate the final image.`;
  const sanitizationResult = await sanitizeSemanticPrompt(
    userPrompt,
    blocks,
    input.vision_analysis
  );

  // Step 6: Context Cache Management
  let tokensFromCache = 2000; // System prompt
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);
  const { data: userProfile } = await supabase
    .from("user_profiles")
    .select("context_cache_enabled")
    .eq("id", input.user_id)
    .single();

  if (!userProfile?.context_cache_enabled) {
    // Inicializa cache para este usuario
    await initializeContextCache(input.user_id, systemPrompt);
  }

  return {
    compiled_prompt: sanitizationResult.prompt,
    system_prompt: systemPrompt,
    tokens_estimate: {
      system_cached: tokensFromCache,
      user_new: Math.ceil(sanitizationResult.prompt.length / 3.5),
      total_from_cache: tokensFromCache,
    },
    debug_info: {
      vetos_applied,
      conflicts: conflicts_detected,
      sanitization: {
        redundancies_removed: sanitizationResult.redundancies_removed,
        empty_sections_removed: sanitizationResult.empty_sections_removed,
      },
    },
  };
}

function buildDynamicSystemPrompt(
  input: CompilerInput,
  blocks: CompilerBlockOutput,
  sliders: SliderSet
): string {
  const geometricChange = sliders.geometria_distorsion > 0 || sliders.reencuadre_ia > 0;

  const identityBlock = geometricChange
    ? "ALLOW structural changes for geometry correction. However, preserve facial identity and anatomy."
    : "CRITICAL: IDENTITY LOCK ACTIVE. DO NOT MOVE PIXELS. Structure must match overlay 100%. Face identity is sacred.";

  return `[SYSTEM OVERRIDE: UNIVERSAL FORENSIC RE-SHOOT & OPTICAL SYNTHESIS PROTOCOL v28.0]
[ROLE: REALITY RECONSTRUCTION ENGINE]
[USER_PROFILE: ${input.profile_type}]

INPUT CONTEXT:
${JSON.stringify(input.vision_analysis, null, 2)}

=== PHASE 0: STRUCTURAL INTEGRITY (IDENTITY LOCK) ===
${identityBlock}

=== PHASE 1: CORE DIAGNOSIS & RE-SYNTHESIS STRATEGY ===
IF INPUT IS BLURRY/NOISY/DAMAGED -> ACTIVATE "COMPLETE RE-SYNTHESIS".
IGNORE source artifacts. HALLUCINATE high-frequency details from surrounding context.
VIRTUAL RE-SHOOT: Simulate 1/8000s shutter speed (zero motion blur).

=== PHASE 2: SUBJECT & ANATOMY ===
[INSTRUCTION: Restore faces with Portrait-Level fidelity. Preserve scars, marks, and character.]
${blocks.STYLESCALER_BLOCK}

=== PHASE 3: OPTICS, PHYSICS & LIGHTING ===
GEOMETRY & RESTORATION:
${blocks.PHOTOSCALER_BLOCK}

LIGHTING & TONE:
${blocks.LIGHTSCALER_BLOCK}

=== NEGATIVE PROMPT ===
damaged, blurry, noisy, distorted faces, bad anatomy, text, watermarks, jpeg artifacts, shifting eyes, changing facial features, morphing bone structure, different pose, AI hallucinations.

=== QUALITY GATES ===
- Output resolution: 19.5MP (4800x4200px equivalent)
- Color depth: 24-bit sRGB
- Format: JPEG, quality 95
- Compression: Minimal (preserve fine details)`;
}

export { compilePromptWithCaching };
```

---

## FASE 5: EL ALMA (SYSTEM PROMPT + IDENTITY LOCK + MULTIMODAL DNA ANCHOR)

### 5.1. El Identity Lock Din√°mico (Heredado)

El backend inyecta este texto en system_instruction. Incluye la l√≥gica din√°mica de integridad.

```typescript
// File: backend/services/identity-lock.ts

interface IdentityLockContext {
  has_face: boolean;
  face_crop_url?: string;
  facial_marks: string[];
  requires_structural_preservation: boolean;
  geometric_changes_enabled: boolean;
}

function generateIdentityLockBlock(
  context: IdentityLockContext
): string {
  if (!context.has_face || !context.requires_structural_preservation) {
    return "Standard processing. No identity constraints.";
  }

  const baseBlock = context.geometric_changes_enabled
    ? `ALLOW structural changes for geometry correction (lens distortion, perspective).
    HOWEVER: Facial identity must be preserved. Bone structure, proportions, character marks are sacred.
    Changes allowed: Lens distortion correction, perspective fixing.
    Changes FORBIDDEN: Changing face shape, proportions, identity markers.`
    : `CRITICAL: IDENTITY LOCK ACTIVE - MAXIMUM CONSTRAINT.
    DO NOT MOVE PIXELS related to face/body structure.
    Structure must match overlay 100%.
    Allowed changes: Color correction, tone mapping, lighting simulation.
    Forbidden changes: Any structural pixel movement, facial morphing, identity alteration.`;

  const marksPreservation =
    context.facial_marks.length > 0
      ? `Facial marks to preserve identically: ${context.facial_marks.join(", ")}`
      : "";

  return [baseBlock, marksPreservation].filter(Boolean).join("\n");
}

export { generateIdentityLockBlock };
```

### 5.2. Multimodal DNA Anchor (NUEVA CARACTER√çSTICA v28.0)

El Identity Lock actual es texto. Si la temperatura creativa es alta, el modelo puede ignorarlo. La soluci√≥n: inyectar biom√©tricamente la cara original como imagen.

#### Concepto de DNA Anchor

```
Fase 1: Detecta cara en imagen normalizada
    ‚Üì
Step 1: Hace crop facial (face_crop.jpg, 256x256 o m√°s)
    ‚Üì
Step 2: Almacena en Supabase Storage
    ‚Üì
Fase 5: Al compilar el prompt, inyecta DOS im√°genes:
    - Imagen A: El lienzo (composici√≥n, iluminaci√≥n)
    - Imagen B: El face_crop.jpg (identidad biom√©trica)
    ‚Üì
Prompt: "Use Image A for lighting/composition.
         Use Image B as the ABSOLUTE BIOMETRIC GROUND TRUTH.
         Structure must match Image B pixel-perfectly."
    ‚Üì
Output: Immuno-genaro a deformaci√≥n facial
```

#### Implementaci√≥n

```typescript
// File: backend/services/dna-anchor-generator.ts

import * as faceapi from "@vladmandic/face-api";
import Jimp from "jimp";

interface DNAAnchor {
  face_detected: boolean;
  face_crop_url?: string;
  face_crop_storage_path?: string;
  face_bounding_box?: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  anchor_strength: "weak" | "medium" | "strong" | "absolute";
}

async function generateDNAAnchor(
  imageBuffer: Buffer,
  jobId: string
): Promise<DNAAnchor> {
  // Step 1: Carga modelo face-api
  await faceapi.nets.tinyFaceDetector.load();
  await faceapi.nets.faceLandmark68Net.load();

  // Step 2: Detecta cara
  const detections = await faceapi.detectAllFaces(imageBuffer);

  if (detections.length === 0) {
    return {
      face_detected: false,
      anchor_strength: "weak",
    };
  }

  // Step 3: Toma la cara m√°s grande/confiable
  const primaryFace = detections[0];
  const box = primaryFace.detection.box;

  // Step 4: Crea un crop con margen (20% extra)
  const margin = 0.2;
  const cropX = Math.max(0, box.x - box.width * margin);
  const cropY = Math.max(0, box.y - box.height * margin);
  const cropWidth = box.width * (1 + 2 * margin);
  const cropHeight = box.height * (1 + 2 * margin);

  const image = await Jimp.read(imageBuffer);
  const cropped = image.crop({
    x: Math.round(cropX),
    y: Math.round(cropY),
    w: Math.round(cropWidth),
    h: Math.round(cropHeight),
  });

  // Step 5: Redimensiona a resoluci√≥n est√°ndar (256x256)
  const resized = await cropped.resize(256, 256);
  const faceCropBuffer = await resized.quality(95).getBuffer("image/jpeg");

  // Step 6: Almacena en Supabase Storage
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);
  const storagePath = `dna-anchors/${new Date().toISOString().split("T")[0]}/${jobId}-face.jpg`;

  const { error: uploadError } = await supabase.storage
    .from("processing-bucket")
    .upload(storagePath, faceCropBuffer, {
      contentType: "image/jpeg",
      metadata: { job_id: jobId, anchor_type: "biometric" },
    });

  if (uploadError) throw uploadError;

  // Step 7: Obtiene URL p√∫blica
  const { data: urlData } = supabase.storage
    .from("processing-bucket")
    .getPublicUrl(storagePath);

  return {
    face_detected: true,
    face_crop_url: urlData.publicUrl,
    face_crop_storage_path: storagePath,
    face_bounding_box: {
      x: Math.round(box.x),
      y: Math.round(box.y),
      width: Math.round(box.width),
      height: Math.round(box.height),
    },
    anchor_strength: "absolute", // DNA Anchor es m√°xima constraintt
  };
}

export { generateDNAAnchor };
```

#### Inyecci√≥n en Prompt (Multimodal)

```typescript
// File: backend/services/multimodal-prompt-injector.ts

interface MultimodalPromptContent {
  parts: Array<{
    text?: string;
    inlineData?: {
      data: string; // base64
      mimeType: string;
    };
  }>;
}

async function buildMultimodalPromptWithDNAAnchor(
  userPrompt: string,
  mainImageBase64: string,
  dnaAnchorUrl?: string
): Promise<MultimodalPromptContent> {
  const parts: MultimodalPromptContent["parts"] = [];

  // Part 1: System instructions
  parts.push({
    text: userPrompt,
  });

  // Part 2: Main image (lienzo)
  parts.push({
    inlineData: {
      data: mainImageBase64,
      mimeType: "image/jpeg",
    },
  });

  // Part 3: DNA Anchor (identidad biom√©trica)
  if (dnaAnchorUrl) {
    // Descarga DNA Anchor
    const response = await fetch(dnaAnchorUrl);
    const buffer = await response.arrayBuffer();
    const base64Data = Buffer.from(buffer).toString("base64");

    parts.push({
      inlineData: {
        data: base64Data,
        mimeType: "image/jpeg",
      },
    });

    // A√±ade instrucci√≥n adicional
    parts.push({
      text: `[BIOMETRIC GROUND TRUTH - IMAGE 3]:
This is the original face/subject identity (DNA Anchor).
When processing Image 1 (main canvas):
- Use Image 1 for lighting, composition, and context.
- Use Image 3 as the ABSOLUTE BIOMETRIC REFERENCE.
- Ensure facial structure matches Image 3 pixel-perfectly.
- No morphing, no identity alteration.
- Preserve all facial marks, scars, and character from Image 3.`,
    });
  }

  return { parts };
}

export { buildMultimodalPromptWithDNAAnchor };
```

---

## FASE 6: REFINEMENT LOOP (CODE EXECUTION MASKING)

### 6.1. El Problema de v27.2

Pedirle a una IA que "imagine" una m√°scara de inpainting (blanco/negro) es impreciso. Las m√°scaras generadas suelen tener:
- Bordes borrosos (feathering no deseado)
- Fugas (p√≠xeles mal clasificados)
- Imprecisi√≥n espacial

### 6.2. La Soluci√≥n v28.0: Code Execution

Gemini 3 Pro soporta "Code Execution" (ejecutar c√≥digo Python en sandbox). En lugar de generar m√°scaras visualmente, el modelo escribe Python que las genera matem√°ticamente.

#### Flujo de Refinement

```
Usuario: "Quitar gafas"
    ‚Üì
Backend: Detecta que es refinement (no cambio de slider)
    ‚Üì
Llama a Code Execution Service
    ‚Üì
Prompt a Gemini: "Write Python code using OpenCV and NumPy.
                 Task: Detect eyeglasses in this image and create a binary mask.
                 Input: input_image.jpg
                 Output: mask.png (white = area to inpaint, black = preserve)"
    ‚Üì
Gemini responde con c√≥digo:
```python
import cv2
import numpy as np
from PIL import Image

# Carga imagen
img = cv2.imread('input_image.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Detecta gafas usando Haar Cascade
glasses_cascade = cv2.CascadeClassifier(
    cv2.data.haarcascades + 'haarcascade_eye_tree_eyeglasses.xml'
)
glasses = glasses_cascade.detectMultiScale(gray, 1.3, 5)

# Crea m√°scara binaria
mask = np.zeros(gray.shape, dtype=np.uint8)
for (x, y, w, h) in glasses:
    cv2.rectangle(mask, (x, y), (x+w, y+h), 255, -1)
    # Dilataci√≥n suave para bordes naturales
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    mask = cv2.dilate(mask, kernel, iterations=2)

# Guarda m√°scara
cv2.imwrite('mask.png', mask)
```
    ‚Üì
Sandbox ejecuta c√≥digo
    ‚Üì
Devuelve mask.png (matriz binaria perfecta)
    ‚Üì
Inpainting quir√∫rgico con esa m√°scara
    ‚Üì
Blending suave (Alpha Blending 0.95)
    ‚Üì
Output: Gafas removidas sin artefactos
```

#### Implementaci√≥n

```typescript
// File: backend/services/code-execution-masking.ts

interface RefinementRequest {
  image_url: string;
  instruction: string; // "Remove glasses", "Remove watermark", etc.
  refinement_type: "object_removal" | "area_inpainting" | "custom";
}

interface MaskingResult {
  mask_url: string;
  mask_type: "binary" | "soft";
  code_executed: string;
  execution_time_ms: number;
  confidence: number;
}

async function generateMaskWithCodeExecution(
  refinementRequest: RefinementRequest
): Promise<MaskingResult> {
  const vertex = new VertexAI({
    project: process.env.GCP_PROJECT_ID,
    location: "europe-west1",
  });

  const model = vertex.getGenerativeModel({
    model: "gemini-3-pro",
  });

  // Step 1: Prompt para generar c√≥digo
  const codeGenerationPrompt = `You are an expert Python developer using OpenCV, NumPy, and PIL.

Task: Write Python code to create a precise binary mask for: "${refinementRequest.instruction}"

Requirements:
- Load image from 'input_image.jpg'
- Create a binary mask (white = area to refine/inpaint, black = preserve)
- Save mask to 'mask.png'
- Use OpenCV for detection (Haar Cascades, edge detection, contour analysis)
- Smooth edges with Gaussian blur (sigma=1.5) for natural inpainting
- Return only Python code, no explanations

Do NOT import requests, urllib, or network libraries. Use only: cv2, numpy, PIL, os, sys.`;

  // Step 2: Llama a Gemini para generar c√≥digo
  const codeResponse = await model.generateContent({
    contents: [
      {
        role: "user",
        parts: [{ text: codeGenerationPrompt }],
      },
    ],
  });

  let pythonCode = codeResponse.response.text();

  // Limpia markdown si viene envuelto
  pythonCode = pythonCode
    .replace(/```python\n?/g, "")
    .replace(/```\n?/g, "");

  console.log("[CODE EXECUTION] Generated Python code:", pythonCode.substring(0, 500));

  // Step 3: Descarga imagen de input
  const imageResponse = await fetch(refinementRequest.image_url);
  const imageBuffer = await imageResponse.arrayBuffer();

  // Step 4: Ejecuta c√≥digo en sandbox de Gemini
  const executionStartTime = Date.now();

  const executionResponse = await model.generateContent({
    contents: [
      {
        role: "user",
        parts: [
          {
            text: `Execute this Python code in a safe sandbox. Upload 'input_image.jpg' and execute the script. Return the base64-encoded output from 'mask.png'.

Python code:
${pythonCode}`,
          },
          {
            inlineData: {
              data: Buffer.from(imageBuffer).toString("base64"),
              mimeType: "image/jpeg",
            },
          },
        ],
      },
    ],
  });

  const executionTime = Date.now() - executionStartTime;
  const executionOutput = executionResponse.response.text();

  // Step 5: Extrae m√°scara base64 del output
  const base64Match = executionOutput.match(
    /data:image\/png;base64,([A-Za-z0-9+/=]+)/
  );
  if (!base64Match) {
    throw new Error("Code execution did not return image data");
  }

  const maskBase64 = base64Match[1];
  const maskBuffer = Buffer.from(maskBase64, "base64");

  // Step 6: Almacena m√°scara en Storage
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);
  const maskStoragePath = `refinement-masks/${Date.now()}-mask.png`;

  const { error: uploadError } = await supabase.storage
    .from("processing-bucket")
    .upload(maskStoragePath, maskBuffer, {
      contentType: "image/png",
    });

  if (uploadError) throw uploadError;

  const { data: urlData } = supabase.storage
    .from("processing-bucket")
    .getPublicUrl(maskStoragePath);

  return {
    mask_url: urlData.publicUrl,
    mask_type: "binary",
    code_executed: pythonCode,
    execution_time_ms: executionTime,
    confidence: 0.95, // Code execution es determinista
  };
}

export { generateMaskWithCodeExecution };
```

#### UI del Usuario: C√≥mo Solicitar Refinement

El usuario debe poder pedir refinement despu√©s de generar la imagen.

```tsx
// File: frontend/components/RefinementDialog.tsx

import React, { useState } from "react";
import { Button } from "@/components/ui/button";
import { Textarea } from "@/components/ui/textarea";

interface RefinementDialogProps {
  generatedImageUrl: string;
  jobId: string;
  onSubmit: (instruction: string) => Promise<void>;
}

export function RefinementDialog({
  generatedImageUrl,
  jobId,
  onSubmit,
}: RefinementDialogProps) {
  const [instruction, setInstruction] = useState("");
  const [isLoading, setIsLoading] = useState(false);

  const commonRefinements = [
    { label: "Remove glasses", value: "Remove eyeglasses" },
    { label: "Remove watermark", value: "Remove watermark" },
    { label: "Remove text", value: "Remove text overlay" },
    { label: "Remove blemish", value: "Remove skin blemish" },
    { label: "Remove background object", value: "Remove unwanted object from background" },
  ];

  const handleSubmit = async () => {
    setIsLoading(true);
    try {
      await onSubmit(instruction);
      setInstruction("");
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="space-y-4">
      <div>
        <h3 className="text-lg font-semibold mb-2">‚ú® Refine Your Image</h3>
        <p className="text-sm text-gray-600 mb-4">
          Use Code Execution Masking to remove objects with surgical precision.
        </p>
      </div>

      <div className="grid grid-cols-2 gap-2">
        {commonRefinements.map((refinement) => (
          <Button
            key={refinement.value}
            variant="outline"
            size="sm"
            onClick={() => setInstruction(refinement.value)}
            className="justify-start"
          >
            {refinement.label}
          </Button>
        ))}
      </div>

      <Textarea
        placeholder="Or write a custom refinement instruction..."
        value={instruction}
        onChange={(e) => setInstruction(e.target.value)}
        className="min-h-24"
      />

      <Button
        onClick={handleSubmit}
        disabled={!instruction || isLoading}
        className="w-full"
      >
        {isLoading ? "Processing..." : "Apply Refinement"}
      </Button>

      <div className="p-3 bg-blue-50 rounded-lg text-sm">
        <p className="font-semibold text-blue-900">üí° Tip:</p>
        <p className="text-blue-800 mt-1">
          Be specific: "Remove glasses but keep the frame shadow" works better than just "Remove glasses".
        </p>
      </div>
    </div>
  );
}
```

---

## INFRAESTRUCTURA: PATR√ìN AS√çNCRONO CON JOB QUEUES

### El Problema de v27.2: Bloqueo HTTP

En v27.2, el flujo era:
```
Cliente HTTP ‚Üí Deno Edge (espera 20-40s) ‚Üí Gemini 3 Pro (espera 20-40s) ‚Üí Timeout 504
```

El cliente est√° bloqueado. Si el servidor/red falla, error.

### La Soluci√≥n v28.0: Job Queues + Realtime

```
Cliente HTTP ‚Üí Deno Edge (< 100ms) ‚Üí HTTP 200 + job_id
    ‚Üì
Cliente recibe job_id inmediatamente
    ‚Üì
Deno Edge encola tarea en Cloud Tasks
    ‚Üì
Cloud Run Worker (background, timeout 60min) procesa
    ‚Üì
Almacena resultado en Supabase
    ‚Üì
Supabase Realtime notifica cliente (WebSocket)
    ‚Üì
Cliente recibe imagen sin esperar
```

### Arquitectura de Encolado

```
Componente 1: API Gateway (Cloudflare Worker)
    - Recibe petici√≥n
    - Genera UUID para job_id
    - Valida autorizaci√≥n
    - Entra a Cloud Tasks
    - Responde HTTP 200 inmediato

Componente 2: Cloud Tasks Queue
    - Mantiene peticiones en cola
    - Reintentos autom√°ticos
    - Dead letter queue para errores

Componente 3: Cloud Run Worker
    - Procesa en background
    - Accede a Vertex AI (Context Cache)
    - Ejecuta Code Execution
    - Guarda resultado en Supabase

Componente 4: Supabase Realtime
    - Client se suscribe a job_id
    - Recibe actualizaci√≥n cuando job completa
    - UI actualiza imagen
```

#### Implementaci√≥n: API Gateway

```typescript
// File: backend/cloudflare-worker/api-gateway.ts

import { Router } from "itty-router";
import { createClient } from "@supabase/supabase-js";
import { CloudTasksClient } from "@google-cloud/tasks";

const router = Router();
const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);
const tasksClient = new CloudTasksClient();

router.post("/api/process-image", async (req) => {
  // Step 1: Validaci√≥n y autenticaci√≥n
  const authHeader = req.headers.get("Authorization");
  const token = authHeader?.split(" ")[1];

  if (!token) {
    return new Response(JSON.stringify({ error: "Unauthorized" }), {
      status: 401,
    });
  }

  const { data: user, error: authError } = await supabase.auth.getUser(token);
  if (authError || !user) {
    return new Response(JSON.stringify({ error: "Invalid token" }), {
      status: 401,
    });
  }

  // Step 2: Parsea request
  const formData = await req.formData();
  const imageFile = formData.get("image") as File;
  const sliderValues = JSON.parse(formData.get("sliders") as string);
  const profileType = formData.get("profile") as string;

  if (!imageFile) {
    return new Response(JSON.stringify({ error: "No image provided" }), {
      status: 400,
    });
  }

  // Step 3: Crea entry en processing_jobs
  const jobId = crypto.randomUUID();
  const { error: jobError } = await supabase.from("processing_jobs").insert({
    id: jobId,
    user_id: user.id,
    status: "QUEUED",
    mode: "MASTER",
    slider_values: sliderValues,
    profile_type: profileType,
    input_path: `temp/${jobId}.jpg`,
  });

  if (jobError) {
    return new Response(
      JSON.stringify({ error: "Failed to create job", details: jobError }),
      { status: 500 }
    );
  }

  // Step 4: Almacena imagen temporalmente
  const imageBuffer = await imageFile.arrayBuffer();
  const { error: storageError } = await supabase.storage
    .from("processing-bucket")
    .upload(`temp/${jobId}.jpg`, imageBuffer, {
      contentType: "image/jpeg",
    });

  if (storageError) {
    return new Response(
      JSON.stringify({ error: "Failed to store image", details: storageError }),
      { status: 500 }
    );
  }

  // Step 5: Encola tarea en Cloud Tasks
  const project = process.env.GCP_PROJECT_ID;
  const queue = "luxscaler-processing-queue";
  const location = "europe-west1";
  const parent = tasksClient.queuePath(project, location, queue);

  const task = {
    httpRequest: {
      httpMethod: "POST",
      url: `${process.env.CLOUD_RUN_URL}/process-job`,
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${process.env.CLOUD_RUN_SERVICE_ACCOUNT_TOKEN}`,
      },
      body: Buffer.from(
        JSON.stringify({
          job_id: jobId,
          user_id: user.id,
          slider_values: sliderValues,
          profile_type: profileType,
        })
      ).toString("base64"),
    },
  };

  const request = { parent, task };

  try {
    const [response] = await tasksClient.createTask(request);
    console.log(`Tarea encolada: ${response.name}`);

    // Step 6: Responde al cliente inmediatamente
    return new Response(
      JSON.stringify({
        job_id: jobId,
        status: "QUEUED",
        estimated_wait_seconds: 5,
      }),
      {
        status: 200,
        headers: { "Content-Type": "application/json" },
      }
    );
  } catch (error) {
    return new Response(
      JSON.stringify({ error: "Failed to enqueue task", details: error }),
      { status: 500 }
    );
  }
});

export default router;
```

#### Implementaci√≥n: Cloud Run Worker

```typescript
// File: backend/cloud-run-worker/main.ts

import Hono from "hono";
import { createClient } from "@supabase/supabase-js";
import { compilePromptWithCaching } from "../services/prompt-compiler";
import { analyzeWithProxyVision } from "../services/proxy-vision-analyzer";
import { normalizeInput } from "../services/input-normalizer";
import { generateDNAAnchor } from "../services/dna-anchor-generator";
import { VertexAI } from "@google-cloud/vertexai";

const app = new Hono();
const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

app.post("/process-job", async (c) => {
  const job = await c.req.json();
  const { job_id, user_id, slider_values, profile_type } = job;

  console.log(`[CLOUD_RUN] Processing job: ${job_id}`);

  try {
    // Step 1: Actualiza status a PROCESSING
    await supabase
      .from("processing_jobs")
      .update({ status: "PROCESSING" })
      .eq("id", job_id);

    await supabase.realtime
      .broadcast(`job:${job_id}`, { status: "PROCESSING", message: "Starting image analysis..." });

    // Step 2: Descarga imagen normalizada
    const { data: imageBuffer, error: downloadError } = await supabase.storage
      .from("processing-bucket")
      .download(`temp/${job_id}.jpg`);

    if (downloadError) throw downloadError;

    // Step 3: Normaliza entrada (Fase 1)
    const { normalized, metadata: normalizationMetadata } =
      await normalizeInput(imageBuffer);

    await supabase.realtime
      .broadcast(`job:${job_id}`, { status: "PROCESSING", message: "Vision analysis in progress..." });

    // Step 4: Almacena imagen normalizada
    const { data: normStorageData } = await supabase.storage
      .from("processing-bucket")
      .upload(`normalized/${job_id}.jpg`, normalized, {
        contentType: "image/jpeg",
      });

    // Step 5: An√°lisis de Vision (Fase 2)
    const visionAnalysis = await analyzeWithProxyVision(
      normStorageData?.path,
      job_id
    );

    // Step 6: Genera DNA Anchor (Multimodal)
    const dnaAnchor = await generateDNAAnchor(normalized, job_id);

    await supabase.realtime
      .broadcast(`job:${job_id}`, { status: "PROCESSING", message: "Compiling prompt..." });

    // Step 7: Compila prompt (Fase 4 + Context Caching)
    const compiledPrompt = await compilePromptWithCaching({
      slider_values,
      vision_analysis: visionAnalysis,
      user_id,
      profile_type,
    });

    await supabase.realtime
      .broadcast(`job:${job_id}`, { status: "PROCESSING", message: "Generating image..." });

    // Step 8: Llama a Gemini 3 Pro (con cache)
    const vertex = new VertexAI({
      project: process.env.GCP_PROJECT_ID,
      location: "europe-west1",
    });

    const model = vertex.getGenerativeModel({
      model: "gemini-3-pro-vision",
    });

    const multimodalContent = dnaAnchor.face_detected
      ? {
          parts: [
            { text: compiledPrompt.compiled_prompt },
            {
              inlineData: {
                data: normalized.toString("base64"),
                mimeType: "image/jpeg",
              },
            },
            {
              inlineData: {
                data: (await fetch(dnaAnchor.face_crop_url!)).arrayBuffer().then(b =>
                  Buffer.from(b).toString("base64")
                ),
                mimeType: "image/jpeg",
              },
            },
          ],
        }
      : {
          parts: [
            { text: compiledPrompt.compiled_prompt },
            {
              inlineData: {
                data: normalized.toString("base64"),
                mimeType: "image/jpeg",
              },
            },
          ],
        };

    const response = await model.generateContent({
      contents: [{ role: "user", parts: multimodalContent.parts }],
      systemInstruction: compiledPrompt.system_prompt,
    });

    const generatedImage = await response.response.text(); // URL base64 de la imagen

    // Step 9: Almacena resultado
    const outputBuffer = Buffer.from(
      generatedImage.split(",")[1] || generatedImage,
      "base64"
    );
    const outputPath = `results/${job_id}.jpg`;

    await supabase.storage
      .from("processing-bucket")
      .upload(outputPath, outputBuffer, { contentType: "image/jpeg" });

    const { data: outputUrl } = supabase.storage
      .from("processing-bucket")
      .getPublicUrl(outputPath);

    // Step 10: Actualiza job a COMPLETED
    await supabase
      .from("processing_jobs")
      .update({
        status: "COMPLETED",
        output_url: outputUrl.publicUrl,
        logs: {
          normalization_metadata: normalizationMetadata,
          vision_analysis: visionAnalysis,
          dna_anchor: dnaAnchor,
          compiler_tokens: compiledPrompt.tokens_estimate,
          compilation_debug: compiledPrompt.debug_info,
        },
      })
      .eq("id", job_id);

    // Step 11: Notifica cliente via Realtime
    await supabase.realtime
      .broadcast(`job:${job_id}`, {
        status: "COMPLETED",
        message: "Image ready!",
        output_url: outputUrl.publicUrl,
      });

    console.log(`[CLOUD_RUN] Job ${job_id} completed successfully`);
  } catch (error) {
    console.error(`[CLOUD_RUN] Job ${job_id} failed:`, error);

    await supabase
      .from("processing_jobs")
      .update({
        status: "FAILED",
        logs: { error: JSON.stringify(error) },
      })
      .eq("id", job_id);

    await supabase.realtime
      .broadcast(`job:${job_id}`, {
        status: "FAILED",
        message: `Error: ${error.message}`,
      });
  }
});

export default app;
```

---

## OPTIMIZACI√ìN: CONTEXT CACHING & PROXY VISION

(Detallado en Fase 2 y Fase 4)

---

## UX: RENDERIZADO H√çBRIDO (PREVIEW VS MASTER)

### El Concepto

Usar el motor "Forensic" (Gemini 3 Pro) es muy poderoso pero lento. Usar "Speed Engine" (Gemini 2.5 Flash) es r√°pido pero menos preciso.

**La soluci√≥n v28.0:** Renderizado en dos velocidades.

### Pipeline de Dos Velocidades

#### MODO LIVE (Interactivo)

```
Usuario mueve slider
    ‚Üì
Frontend valida (no env√≠a 0)
    ‚Üì
Cloudflare Worker ‚Üí HTTP request
    ‚Üì
Deno Edge: Crea mini-job PREVIEW
    ‚Üì
Cloud Run (lightweight worker)
    ‚Üì
Gemini 2.5 Flash (velocidad)
    ‚Üì
Resoluci√≥n: 1024px
    ‚Üì
Prompt: Simplificado (1/3 del tama√±o)
    ‚Üì
Latencia: <2s
    ‚Üì
Respuesta: thumbnail actualizado en canvas
```

#### MODO MASTER (Exportar/Finalizar)

```
Usuario presiona "Generate Final"
    ‚Üì
Frontend env√≠a todo estado de sliders
    ‚Üì
Cloudflare Worker ‚Üí HTTP request
    ‚Üì
Deno Edge: Crea job MASTER
    ‚Üì
Cloud Run (full worker)
    ‚Üì
Fase completa 1-6
    ‚Üì
Gemini 3 Pro (precisi√≥n forense)
    ‚Üì
Resoluci√≥n: 19.5MP (4800x4200px)
    ‚Üì
Prompt: Full compilado + Context Caching
    ‚Üì
Latencia: 20-40s (as√≠ncrono, no bloquea)
    ‚Üì
Respuesta: Supabase Realtime notificaci√≥n
    ‚Üì
Cliente recibe imagen final
```

#### Implementaci√≥n Frontend

```tsx
// File: frontend/components/HybridRenderingCanvas.tsx

import React, { useState, useEffect } from "react";
import { useLuxScalerStore } from "@/stores/luxscaler-store";
import { useRealtimeSubscription } from "@/hooks/useRealtimeSubscription";

interface RenderState {
  mode: "PREVIEW" | "MASTER";
  status: "idle" | "rendering" | "completed" | "error";
  previewUrl?: string;
  masterUrl?: string;
  jobId?: string;
  estimatedTime?: number;
}

export function HybridRenderingCanvas() {
  const { sliders, updateSlider } = useLuxScalerStore();
  const [renderState, setRenderState] = useState<RenderState>({
    mode: "PREVIEW",
    status: "idle",
  });
  const [debounceTimer, setDebounceTimer] = useState<NodeJS.Timeout>();

  // Suscripci√≥n a Realtime para MASTER
  const { data: realtimeUpdate } = useRealtimeSubscription(renderState.jobId);

  // Step 1: Slider cambi√≥ ‚Üí Renderiza PREVIEW
  const handleSliderChange = (sliderName: string, value: number) => {
    updateSlider(sliderName, value);

    // Debounce: espera 300ms antes de renderizar
    if (debounceTimer) clearTimeout(debounceTimer);

    const timer = setTimeout(async () => {
      setRenderState((prev) => ({
        ...prev,
        mode: "PREVIEW",
        status: "rendering",
      }));

      try {
        const response = await fetch("/api/preview", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            sliders: sliders,
            mode: "PREVIEW",
          }),
        });

        if (!response.ok) throw new Error("Preview render failed");

        const { preview_url } = await response.json();

        setRenderState((prev) => ({
          ...prev,
          previewUrl: preview_url,
          status: "completed",
        }));
      } catch (error) {
        setRenderState((prev) => ({
          ...prev,
          status: "error",
        }));
      }
    }, 300);

    setDebounceTimer(timer);
  };

  // Step 2: Usuario presiona "Generate Final" ‚Üí Renderiza MASTER
  const handleGenerateFinal = async () => {
    setRenderState((prev) => ({
      ...prev,
      mode: "MASTER",
      status: "rendering",
    }));

    try {
      const response = await fetch("/api/generate-master", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          sliders: sliders,
          mode: "MASTER",
        }),
      });

      if (!response.ok) throw new Error("Master generation failed");

      const { job_id, estimated_wait_seconds } = await response.json();

      setRenderState((prev) => ({
        ...prev,
        jobId: job_id,
        estimatedTime: estimated_wait_seconds,
      }));

      // Espera por Realtime update
    } catch (error) {
      setRenderState((prev) => ({
        ...prev,
        status: "error",
      }));
    }
  };

  // Step 3: Realtime update cuando MASTER completa
  useEffect(() => {
    if (realtimeUpdate?.status === "COMPLETED") {
      setRenderState((prev) => ({
        ...prev,
        masterUrl: realtimeUpdate.output_url,
        status: "completed",
      }));
    } else if (realtimeUpdate?.status === "FAILED") {
      setRenderState((prev) => ({
        ...prev,
        status: "error",
      }));
    }
  }, [realtimeUpdate]);

  return (
    <div className="space-y-6">
      {/* Sliders */}
      <div className="space-y-3">
        {Object.entries(sliders).map(([name, value]) => (
          <div key={name} className="flex items-center gap-4">
            <label className="w-32 text-sm font-medium">{name}</label>
            <input
              type="range"
              min="0"
              max="10"
              value={value}
              onChange={(e) =>
                handleSliderChange(name, parseInt(e.target.value))
              }
              className="flex-1"
            />
            <span className="w-8 text-right text-sm">{value}</span>
          </div>
        ))}
      </div>

      {/* Canvas */}
      <div className="aspect-video bg-gray-200 rounded-lg overflow-hidden">
        {renderState.previewUrl && (
          <img
            src={renderState.previewUrl}
            alt="Preview"
            className="w-full h-full object-contain"
          />
        )}
        {renderState.masterUrl && (
          <img
            src={renderState.masterUrl}
            alt="Master"
            className="w-full h-full object-contain"
          />
        )}
        {renderState.status === "rendering" && (
          <div className="w-full h-full flex items-center justify-center bg-gray-300">
            <div className="text-center">
              <p className="text-lg font-semibold">
                {renderState.mode === "PREVIEW"
                  ? "Rendering preview..."
                  : "Generating master image..."}
              </p>
              {renderState.estimatedTime && (
                <p className="text-sm text-gray-600 mt-2">
                  Estimated time: {renderState.estimatedTime}s
                </p>
              )}
            </div>
          </div>
        )}
      </div>

      {/* Button */}
      <div className="flex gap-4">
        <button
          onClick={handleGenerateFinal}
          disabled={renderState.status === "rendering"}
          className="px-6 py-3 bg-blue-600 text-white rounded-lg font-semibold hover:bg-blue-700 disabled:opacity-50"
        >
          {renderState.status === "rendering"
            ? "Processing..."
            : "Generate Final (Master)"}
        </button>

        {renderState.masterUrl && (
          <a
            href={renderState.masterUrl}
            download="luxscaler-result.jpg"
            className="px-6 py-3 bg-green-600 text-white rounded-lg font-semibold hover:bg-green-700"
          >
            Download
          </a>
        )}
      </div>

      {/* Status */}
      {renderState.status === "error" && (
        <div className="p-4 bg-red-100 text-red-800 rounded-lg">
          <p>Error during rendering. Please try again.</p>
        </div>
      )}
    </div>
  );
}
```

---

## GESTI√ìN DE SMART PRESETS (L√ìGICA DE ARCHIVO + OPACIDAD)

### Concepto Fundamental

Los Presets NO son fotos est√°ticas de todos los sliders. Permiten:
1. **Bloqueo de Pilares:** Solo guarda ciertos pilares (ej: Preset "Studio Lighting" bloquea LIGHTSCALER, deja PHOTOSCALER libre)
2. **Opacidad del Prompt:** El usuario NUNCA sabe cu√°l es el prompt exacto. El preset es una "caja negra"
3. **Restricci√≥n de Compartir:** Solo PROLUX puede compartir presets, y solo con otros PROLUX

### 9.1. L√≥gica de "Anchor Locking"

```sql
CREATE TABLE smart_presets (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES user_profiles(id),
    name VARCHAR(100) NOT NULL,
    description TEXT,

    -- Configuraci√≥n de Sliders (OPACO)
    slider_values JSONB NOT NULL, -- Los valores exactos. Usuario NO ver√° esto directamente.
    compiled_prompt TEXT NOT NULL, -- El prompt exacto compilado. OCULTO.
    vision_analysis_context JSONB, -- Contexto de Vision. OCULTO.

    -- L√≥gica de Bloqueo (Smart Lock)
    locked_pillars TEXT[] NOT NULL, -- ['LIGHTSCALER'] = solo estos pilares se fuerzan
    unlocked_pillars TEXT[] NOT NULL, -- ['PHOTOSCALER', 'STYLESCALER'] = usuario puede variar
    narrative_anchors TEXT[], -- ['dark background', 'rembrandt light'] - hints textuales

    -- Metadata
    preset_type VARCHAR(50) CHECK (preset_type IN ('THEME', 'MACRO', 'STYLE', 'CUSTOM')),
    thumbnail_url TEXT,
    is_public BOOLEAN DEFAULT false, -- ¬øCompartible?
    is_official BOOLEAN DEFAULT false, -- ¬øCreado por equipo LuxScaler?
    usage_count INT DEFAULT 0,
    
    -- Monetizaci√≥n
    min_profile_level VARCHAR(20) CHECK (min_profile_level IN ('USER', 'PRO', 'PROLUX')),
    requires_subscription BOOLEAN DEFAULT false,

    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

### 9.2. Tipos de Presets

| Tipo | Locked Pillars | Unlocked | Descripci√≥n | Visible para |
|---|---|---|---|---|
| **THEME** | 1 pilar completo | 2 pilares | Tema cohesivo. Ej: "Studio Lighting" | ALL |
| **MACRO** | Subset de 1 pilar | El resto | Macro PRO. Ej: "Vintage Aesthetics" | USER, PRO, PROLUX |
| **STYLE** | STYLESCALER | PHOTOSCALER, LIGHTSCALER | Estilo de retoque. Ej: "High-End Luxury" | ALL |
| **CUSTOM** | Personalizado | Personalizado | Usuario guarda su config | USER (solo propia) |

### 9.3. Creaci√≥n de Presets por Usuario

```typescript
// File: backend/services/preset-manager.ts

interface CreatePresetRequest {
  name: string;
  slider_values: SliderSet;
  locked_pillars: string[];
  narrative_anchors?: string[];
  description?: string;
}

async function createSmartPreset(
  userId: string,
  request: CreatePresetRequest
): Promise<{
  preset_id: string;
  locked_pillars: string[];
  unlocked_pillars: string[];
  message: string;
}> {
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

  // Calcula pillares sin bloquear
  const allPillars = ["PHOTOSCALER", "STYLESCALER", "LIGHTSCALER"];
  const unlockedPillars = allPillars.filter(
    (p) => !request.locked_pillars.includes(p)
  );

  // Compila el prompt (pero NO lo muestra al usuario)
  const { translations } = await translateAllSlidersToInstructions(
    request.slider_values
  );
  const blocks = await injectSemanticBlocks(
    request.slider_values,
    translations
  );
  const compiledPrompt = JSON.stringify(blocks, null, 2);

  // Crea entrada en BD
  const { data: preset, error } = await supabase
    .from("smart_presets")
    .insert({
      user_id: userId,
      name: request.name,
      description: request.description,
      slider_values: request.slider_values,
      compiled_prompt: compiledPrompt, // OCULTO
      locked_pillars: request.locked_pillars,
      unlocked_pillars: unlockedPillars,
      narrative_anchors: request.narrative_anchors || [],
      preset_type: "CUSTOM",
      is_public: false,
      min_profile_level: "USER", // El usuario decide
    })
    .select("id")
    .single();

  if (error) throw error;

  return {
    preset_id: preset.id,
    locked_pillars: request.locked_pillars,
    unlocked_pillars: unlockedPillars,
    message: `Preset "${request.name}" guardado. Pillares bloqueados: ${request.locked_pillars.join(", ")}`,
  };
}
```

### 9.4. Aplicaci√≥n de Presets

```typescript
// File: backend/services/preset-applicator.ts

interface PresetApplicationRequest {
  preset_id: string;
  user_overrides?: {
    [sliderName: string]: number; // Usuario puede override sliders desbloqueados
  };
}

async function applySmartPreset(
  userId: string,
  request: PresetApplicationRequest
): Promise<{
  final_slider_values: SliderSet;
  locked_pillars: string[];
  user_can_modify: string[];
  compilation_prompt: string;
}> {
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

  // Step 1: Recupera preset
  const { data: preset, error } = await supabase
    .from("smart_presets")
    .select("*")
    .eq("id", request.preset_id)
    .single();

  if (error) throw error;

  // Step 2: Combina slider_values preestablecidos con overrides del usuario
  let finalSliders = { ...preset.slider_values };

  if (request.user_overrides) {
    for (const [sliderName, value] of Object.entries(
      request.user_overrides
    )) {
      // Valida que el slider sea desbloqueado
      const sliderMapping = await supabase
        .from("slider_semantic_mappings")
        .select("pillar")
        .eq("slider_name", sliderName)
        .single();

      if (
        sliderMapping &&
        preset.unlocked_pillars.includes(sliderMapping.pillar)
      ) {
        finalSliders[sliderName] = value;
      }
      // Si est√° bloqueado, ignora el override
    }
  }

  // Step 3: Devuelve configuraci√≥n final
  return {
    final_slider_values: finalSliders,
    locked_pillars: preset.locked_pillars,
    user_can_modify: preset.unlocked_pillars,
    compilation_prompt: preset.compiled_prompt, // A√∫n oculto
  };
}
```

### 9.5. UI: Selector de Presets

```tsx
// File: frontend/components/PresetSelector.tsx

import React, { useEffect, useState } from "react";
import { useLuxScalerStore } from "@/stores/luxscaler-store";

interface Preset {
  id: string;
  name: string;
  description: string;
  locked_pillars: string[];
  unlocked_pillars: string[];
  thumbnail_url?: string;
}

export function PresetSelector() {
  const { setSliders } = useLuxScalerStore();
  const [presets, setPresets] = useState<Preset[]>([]);
  const [selectedPreset, setSelectedPreset] = useState<string | null>(null);

  useEffect(() => {
    // Carga presets del usuario
    fetch("/api/presets/user").then((res) =>
      res.json().then((data) => setPresets(data))
    );
  }, []);

  const handleApplyPreset = async (presetId: string) => {
    const response = await fetch("/api/presets/apply", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ preset_id: presetId }),
    });

    const { final_slider_values, locked_pillars } = await response.json();

    setSliders(final_slider_values);
    setSelectedPreset(presetId);
  };

  return (
    <div className="space-y-4">
      <h3 className="text-lg font-semibold">Saved Presets</h3>

      <div className="grid grid-cols-2 gap-3">
        {presets.map((preset) => (
          <div
            key={preset.id}
            onClick={() => handleApplyPreset(preset.id)}
            className={`p-4 rounded-lg cursor-pointer border-2 transition ${
              selectedPreset === preset.id
                ? "border-blue-500 bg-blue-50"
                : "border-gray-200 hover:border-gray-400"
            }`}
          >
            {preset.thumbnail_url && (
              <img
                src={preset.thumbnail_url}
                alt={preset.name}
                className="w-full h-24 object-cover rounded mb-2"
              />
            )}
            <p className="font-semibold text-sm">{preset.name}</p>
            <p className="text-xs text-gray-600 mt-1">{preset.description}</p>

            {/* Badges de bloqueo */}
            <div className="mt-2 flex flex-wrap gap-1">
              {preset.locked_pillars.map((pillar) => (
                <span
                  key={pillar}
                  className="inline-block px-2 py-1 bg-blue-100 text-blue-800 rounded text-xs"
                >
                  üîí {pillar.split("_")[0]}
                </span>
              ))}
            </div>
          </div>
        ))}
      </div>

      <button
        onClick={() => {
          /* TODO: Create new preset dialog */
        }}
        className="w-full px-4 py-2 border-2 border-dashed border-gray-300 rounded-lg text-gray-600 hover:border-gray-400"
      >
        + Save Current Configuration as Preset
      </button>
    </div>
  );
}
```

---

## SISTEMA DE MONETIZACI√ìN (RESTRICCIONES POR NIVEL)

### Matriz de Acceso

| Feature | AUTO | USER | PRO | PROLUX |
|---|---|---|---|---|
| **Uso por mes** | 5 | 50 | 500 | Ilimitado |
| **Resoluci√≥n m√°x** | 1024px | 2K | 4K | 19.5MP |
| **Vision Analysis** | ‚úÖ Proxy | ‚úÖ Proxy | ‚úÖ Proxy | ‚úÖ Proxy |
| **PromptCompilerService** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Context Caching** | ‚ùå | ‚úÖ (limitado) | ‚úÖ | ‚úÖ |
| **Code Execution Masking** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| **Multimodal DNA Anchor** | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ |
| **Guardar Presets** | ‚ùå | ‚úÖ (max 5) | ‚úÖ (max 20) | ‚úÖ (ilimitado) |
| **Compartir Presets** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ (solo PROLUX) |
| **Refinement Loop** | ‚ùå | ‚ùå | ‚úÖ (3/mes) | ‚úÖ (ilimitado) |

### Implementaci√≥n: Rate Limiting

```typescript
// File: backend/middleware/rate-limiting.ts

import { createClient } from "@supabase/supabase-js";

async function enforceRateLimit(userId: string, profileType: string) {
  const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

  // L√≠mites por perfil
  const limits = {
    AUTO: 5,
    USER: 50,
    PRO: 500,
    PROLUX: 999999,
  };

  const limit = limits[profileType as keyof typeof limits] || 0;

  // Step 1: Cuenta jobs del mes actual
  const now = new Date();
  const monthStart = new Date(now.getFullYear(), now.getMonth(), 1);

  const { data: jobs, error } = await supabase
    .from("processing_jobs")
    .select("id")
    .eq("user_id", userId)
    .eq("status", "COMPLETED")
    .gte("created_at", monthStart.toISOString());

  if (error) throw error;

  const usedJobs = jobs?.length || 0;

  if (usedJobs >= limit) {
    throw new Error(
      `Monthly limit exceeded. Used: ${usedJobs}/${limit}. Upgrade your plan.`
    );
  }

  return {
    used: usedJobs,
    limit,
    remaining: limit - usedJobs,
  };
}

export { enforceRateLimit };
```

---

## SCHEMA SQL COMPLETO v28.0

```sql
-- ============================================================================
-- LUXSCALER v28.0 COMPLETE DATABASE SCHEMA
-- ============================================================================

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "vector";

-- ============================================================================
-- 1. USER PROFILES (Heredado + Extensiones v28.0)
-- ============================================================================

CREATE TABLE user_profiles (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    email VARCHAR(255) UNIQUE NOT NULL,
    auth_id UUID NOT NULL,
    
    -- Plan/Profile
    profile_type VARCHAR(20) CHECK (profile_type IN ('AUTO', 'USER', 'PRO', 'PROLUX')) DEFAULT 'AUTO',
    monthly_jobs_used INT DEFAULT 0,
    monthly_jobs_limit INT DEFAULT 5, -- Var√≠a seg√∫n profile_type
    
    -- Context Caching (v28.0)
    context_cache_enabled BOOLEAN DEFAULT false,
    context_cache_token TEXT, -- Token de Vertex AI
    context_cache_expires_at TIMESTAMPTZ,
    context_cache_last_initialized_at TIMESTAMPTZ,
    
    -- Presets
    max_presets INT DEFAULT 5, -- USER: 5, PRO: 20, PROLUX: ilimitado
    
    -- Billing
    stripe_customer_id VARCHAR(255),
    subscription_active BOOLEAN DEFAULT false,
    subscription_expires_at TIMESTAMPTZ,
    
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_user_profiles_email ON user_profiles(email);
CREATE INDEX idx_user_profiles_auth_id ON user_profiles(auth_id);

-- ============================================================================
-- 2. PROCESSING JOBS (Patr√≥n As√≠ncrono v28.0)
-- ============================================================================

CREATE TABLE processing_jobs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES user_profiles(id) ON DELETE CASCADE,
    
    -- Status workflow
    status VARCHAR(50) CHECK (status IN ('QUEUED', 'PROCESSING', 'COMPLETED', 'FAILED')) DEFAULT 'QUEUED',
    mode VARCHAR(50) CHECK (mode IN ('PREVIEW', 'MASTER')) DEFAULT 'MASTER',
    
    -- Input configuration
    input_path TEXT NOT NULL, -- Ruta en Storage
    slider_values JSONB NOT NULL, -- Valores de sliders
    profile_type VARCHAR(20) NOT NULL,
    
    -- Vision Analysis (Fase 2)
    vision_analysis_result JSONB,
    proxy_thumbnail_hash VARCHAR(64), -- SHA256 del thumbnail
    
    -- DNA Anchor (Fase 5)
    dna_anchor_url TEXT,
    dna_anchor_storage_path TEXT,
    
    -- Compilation (Fase 4)
    compiled_prompt TEXT,
    prompt_compiler_debug JSONB, -- Vetos, conflictos, etc.
    
    -- Output
    output_url TEXT, -- Imagen final
    output_storage_path TEXT,
    
    -- Logs y observabilidad
    logs JSONB, -- { normalization_metadata, vision_tokens, cache_hits, etc. }
    error_message TEXT,
    execution_time_ms INT,
    
    created_at TIMESTAMPTZ DEFAULT NOW(),
    completed_at TIMESTAMPTZ,
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_processing_jobs_user_id ON processing_jobs(user_id);
CREATE INDEX idx_processing_jobs_status ON processing_jobs(status);
CREATE INDEX idx_processing_jobs_created_at ON processing_jobs(created_at DESC);

-- ============================================================================
-- 3. VISION ANALYSIS CACHE (Optimizaci√≥n Fase 2)
-- ============================================================================

CREATE TABLE vision_analysis_cache (
    image_hash VARCHAR(64) PRIMARY KEY, -- SHA256 del thumbnail
    analysis_payload JSONB NOT NULL, -- {technical_score, semantic_anchors, suggested_settings}
    cache_hits INT DEFAULT 0, -- N√∫mero de veces reutilizado
    created_at TIMESTAMPTZ DEFAULT NOW(),
    expires_at TIMESTAMPTZ DEFAULT (NOW() + INTERVAL '7 days')
);

CREATE INDEX idx_vision_cache_expires_at ON vision_analysis_cache(expires_at);

-- ============================================================================
-- 4. SLIDER SEMANTIC MAPPINGS (Motor Sem√°ntico Fase 3)
-- ============================================================================

CREATE TABLE slider_semantic_mappings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    
    slider_name VARCHAR(100) NOT NULL UNIQUE,
    slider_id INT CHECK (slider_id >= 1 AND slider_id <= 27),
    
    -- Instrucciones por intensidad
    instruction_off TEXT,
    instruction_low TEXT,
    instruction_med TEXT,
    instruction_high TEXT,
    instruction_force TEXT,
    
    -- Metadatos
    pillar VARCHAR(50) CHECK (pillar IN ('PHOTOSCALER', 'STYLESCALER', 'LIGHTSCALER')) NOT NULL,
    semantic_field VARCHAR(100),
    requires_identity_lock BOOLEAN DEFAULT true,
    conflicts_with TEXT[], -- Array de nombres de sliders
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_slider_mappings_pillar ON slider_semantic_mappings(pillar);

-- ============================================================================
-- 5. MACRO DEFINITIONS (PRO Level)
-- ============================================================================

CREATE TABLE macro_definitions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    
    macro_name VARCHAR(100) NOT NULL UNIQUE,
    macro_id INT,
    profile_type VARCHAR(20) CHECK (profile_type IN ('USER', 'PRO')) NOT NULL,
    
    -- Sliders afectados y peso
    affected_sliders JSONB NOT NULL, -- {slider_name: weight (0-1), ...}
    
    -- Descripci√≥n
    description TEXT,
    category VARCHAR(100),
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ============================================================================
-- 6. SMART PRESETS (Gesti√≥n v28.0 + Opacidad)
-- ============================================================================

CREATE TABLE smart_presets (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES user_profiles(id) ON DELETE CASCADE,
    
    -- Metadata
    name VARCHAR(100) NOT NULL,
    description TEXT,
    preset_type VARCHAR(50) CHECK (preset_type IN ('THEME', 'MACRO', 'STYLE', 'CUSTOM')) DEFAULT 'CUSTOM',
    
    -- Configuraci√≥n (OPACA - usuario no ve los valores)
    slider_values JSONB NOT NULL,
    compiled_prompt TEXT NOT NULL, -- OCULTO
    vision_analysis_context JSONB, -- OCULTO
    
    -- Smart Lock
    locked_pillars TEXT[] NOT NULL,
    unlocked_pillars TEXT[] NOT NULL,
    narrative_anchors TEXT[],
    
    -- Thumbnail para preview
    thumbnail_url TEXT,
    
    -- Monetizaci√≥n
    is_public BOOLEAN DEFAULT false,
    is_official BOOLEAN DEFAULT false, -- Creado por LuxScaler team
    usage_count INT DEFAULT 0,
    min_profile_level VARCHAR(20) CHECK (min_profile_level IN ('USER', 'PRO', 'PROLUX')) DEFAULT 'USER',
    requires_subscription BOOLEAN DEFAULT false,
    
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_smart_presets_user_id ON smart_presets(user_id);
CREATE INDEX idx_smart_presets_is_public ON smart_presets(is_public);

-- ============================================================================
-- 7. PRESET SHARING (Solo PROLUX)
-- ============================================================================

CREATE TABLE preset_shares (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    preset_id UUID NOT NULL REFERENCES smart_presets(id) ON DELETE CASCADE,
    shared_by_user_id UUID NOT NULL REFERENCES user_profiles(id) ON DELETE CASCADE,
    shared_with_user_id UUID NOT NULL REFERENCES user_profiles(id) ON DELETE CASCADE,
    
    -- Permisos
    can_modify BOOLEAN DEFAULT false,
    can_reshare BOOLEAN DEFAULT false,
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_preset_shares_preset_id ON preset_shares(preset_id);
CREATE INDEX idx_preset_shares_shared_with ON preset_shares(shared_with_user_id);

-- ============================================================================
-- 8. STYLE EMBEDDINGS (pgvector - Magic Match RAG)
-- ============================================================================

ALTER TABLE smart_presets
ADD COLUMN IF NOT EXISTS style_embedding vector(768);

CREATE INDEX idx_smart_presets_style_embedding ON smart_presets
USING ivfflat (style_embedding vector_cosine_ops)
WITH (lists = 100);

-- ============================================================================
-- 9. REFINEMENT ITERATIONS (Fase 6)
-- ============================================================================

CREATE TABLE refinement_iterations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID NOT NULL REFERENCES processing_jobs(id) ON DELETE CASCADE,
    
    -- Instrucci√≥n de refinement
    refinement_instruction TEXT NOT NULL,
    refinement_type VARCHAR(50) CHECK (refinement_type IN ('object_removal', 'area_inpainting', 'custom')),
    
    -- M√°scara generada (Code Execution)
    mask_url TEXT,
    mask_code TEXT, -- Python code que fue ejecutado
    code_execution_time_ms INT,
    
    -- Output
    output_url TEXT,
    
    -- Metadata
    confidence FLOAT DEFAULT 0.95,
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_refinement_iterations_job_id ON refinement_iterations(job_id);

-- ============================================================================
-- 10. ANALYTICS (Observabilidad)
-- ============================================================================

CREATE TABLE analytics_events (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES user_profiles(id) ON DELETE CASCADE,
    
    event_type VARCHAR(100) NOT NULL, -- 'job_completed', 'preset_saved', etc.
    event_data JSONB,
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_analytics_events_user_id ON analytics_events(user_id);
CREATE INDEX idx_analytics_events_type ON analytics_events(event_type);

-- ============================================================================
-- 11. AUDIT LOG (Compliance)
-- ============================================================================

CREATE TABLE audit_log (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES user_profiles(id) ON DELETE SET NULL,
    
    action VARCHAR(100) NOT NULL, -- 'preset_shared', 'job_completed', etc.
    resource_type VARCHAR(100),
    resource_id UUID,
    
    changes JSONB, -- Antes y despu√©s
    ip_address INET,
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_audit_log_user_id ON audit_log(user_id);
CREATE INDEX idx_audit_log_created_at ON audit_log(created_at DESC);

-- ============================================================================
-- TRIGGERS (Mantenimiento autom√°tico)
-- ============================================================================

-- Actualiza updated_at en user_profiles
CREATE TRIGGER update_user_profiles_updated_at
BEFORE UPDATE ON user_profiles
FOR EACH ROW
EXECUTE FUNCTION update_updated_at_column();

-- Actualiza updated_at en smart_presets
CREATE TRIGGER update_smart_presets_updated_at
BEFORE UPDATE ON smart_presets
FOR EACH ROW
EXECUTE FUNCTION update_updated_at_column();

-- Auto-limpia vision_analysis_cache despu√©s de 7 d√≠as
CREATE TRIGGER delete_expired_vision_cache
AFTER INSERT ON vision_analysis_cache
FOR EACH ROW
EXECUTE FUNCTION delete_expired_cache_records();

-- ============================================================================
-- HELPER FUNCTIONS
-- ============================================================================

CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION delete_expired_cache_records()
RETURNS TRIGGER AS $$
BEGIN
    DELETE FROM vision_analysis_cache WHERE expires_at < NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- VIEWS (Analytics)
-- ============================================================================

CREATE VIEW user_monthly_usage AS
SELECT 
    user_id,
    DATE_TRUNC('month', created_at) as month,
    COUNT(*) as jobs_completed,
    AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_execution_time_sec,
    MAX(completed_at) as last_job_at
FROM processing_jobs
WHERE status = 'COMPLETED'
GROUP BY user_id, DATE_TRUNC('month', created_at);

CREATE VIEW preset_popularity AS
SELECT 
    id,
    name,
    user_id,
    usage_count,
    is_official,
    ROW_NUMBER() OVER (ORDER BY usage_count DESC) as popularity_rank
FROM smart_presets
WHERE is_public = true
ORDER BY usage_count DESC;

-- ============================================================================
-- PERMISSIONS (RBAC)
-- ============================================================================

-- Solo el usuario propietario puede ver sus propios jobs
CREATE POLICY user_jobs_isolation ON processing_jobs
FOR SELECT
USING (user_id = auth.uid());

-- Solo PROLUX puede ver presets p√∫blicos
CREATE POLICY prolux_view_public_presets ON smart_presets
FOR SELECT
USING (
    is_public = true 
    OR user_id = auth.uid()
    OR (
        user_id IN (
            SELECT shared_by_user_id FROM preset_shares
            WHERE shared_with_user_id = auth.uid()
        )
    )
);
```

---

## FLUJO INTEGRADO DE USUARIO (END-TO-END)

### Escenario Completo: Usuario PRO

```
1. Usuario abre LuxScaler Web App
   ‚îî‚îÄ Frontend carga en React 19
   ‚îî‚îÄ Autentica con Supabase Auth
   ‚îî‚îÄ Recupera profile_type: PRO

2. Usuario sube imagen JPG (3MB, 4500x3000 px = 13.5 MP)
   ‚îî‚îÄ Frontend: valida JPEG
   ‚îî‚îÄ POST /api/upload-preview
   ‚îî‚îÄ Cloudflare Worker recibe

3. Cloudflare Worker (Deno Edge)
   ‚îî‚îÄ Valida autenticaci√≥n
   ‚îî‚îÄ Normaliza input (Fase 1)
   ‚îî‚îÄ Genera job_id: "abc123"
   ‚îî‚îÄ Almacena en Supabase Storage
   ‚îî‚îÄ Crea entry en processing_jobs {status: QUEUED}
   ‚îî‚îÄ Encola en Cloud Tasks
   ‚îî‚îÄ HTTP 200: {job_id: "abc123"}

4. Cliente recibe job_id
   ‚îî‚îÄ Suscribe al Realtime: channel "job:abc123"
   ‚îî‚îÄ Muestra canvas en blanco
   ‚îî‚îÄ Muestra estado: "Analyzing..."

5. Cloud Run Worker inicia procesamiento
   ‚îî‚îÄ Descarga imagen normalizada
   ‚îî‚îÄ Fase 1: Input Normalization
       ‚îî‚îÄ Jimp redimensiona a 19.5 MP
       ‚îî‚îÄ Convierte a JPEG sRGB 90
   ‚îî‚îÄ Fase 2: Vision Analysis (Proxy)
       ‚îî‚îÄ Genera thumbnail 1024px
       ‚îî‚îÄ Consulta vision_analysis_cache
       ‚îî‚îÄ Hit: an√°lisis recuperado de cach√©
       ‚îî‚îÄ Devuelve: {technical_score: 6, semantic_anchors: [...]
   ‚îî‚îÄ Fase 5: DNA Anchor
       ‚îî‚îÄ Detecta cara con face-api
       ‚îî‚îÄ Hace crop facial 256x256
       ‚îî‚îÄ Almacena en Storage
   ‚îî‚îÄ Fase 4: Prompt Compilation + Context Caching
       ‚îî‚îÄ Usuario tiene context_cache_enabled: true
       ‚îî‚îÄ Aplica vetos (reglas de conflicto)
       ‚îî‚îÄ Traduce sliders a instrucciones
       ‚îî‚îÄ Inyecta bloques sem√°nticos
       ‚îî‚îÄ Sanitiza prompt
       ‚îî‚îÄ Reutiliza cached system prompt (2000 tokens ahorrados)
   ‚îî‚îÄ Fase 5: Multimodal DNA Anchor Injection
       ‚îî‚îÄ Construye multimodal content con 2 im√°genes:
           - Imagen A: lienzo principal
           - Imagen B: face_crop.jpg (DNA Anchor)
   ‚îî‚îÄ Genera imagen con Gemini 3 Pro
       ‚îî‚îÄ Latencia: 20-40s
   ‚îî‚îÄ Fase 6: Refinement (si usuario selecciona)
       ‚îî‚îÄ N/A en este ejemplo
   ‚îî‚îÄ Actualiza processing_jobs {status: COMPLETED, output_url}
   ‚îî‚îÄ Broadcast en Realtime: {status: COMPLETED, output_url}

6. Cliente recibe notificaci√≥n Realtime
   ‚îî‚îÄ Canvas se actualiza con imagen
   ‚îî‚îÄ Muestra bot√≥n "Download"

7. Usuario (PRO) mueve slider
   ‚îî‚îÄ Frontend debounce 300ms
   ‚îî‚îÄ POST /api/preview (resoluci√≥n 2K)
   ‚îî‚îÄ Cloud Run (lightweight):
       ‚îî‚îÄ Gemini 2.5 Flash
       ‚îî‚îÄ Latencia: <2s
   ‚îî‚îÄ Canvas actualiza con preview

8. Usuario presiona "Generate Final"
   ‚îî‚îÄ POST /api/generate-master
   ‚îî‚îÄ Repite pasos 3-5 pero con MASTER mode
   ‚îî‚îÄ Resultado: 4K image
   ‚îî‚îÄ Supabase Realtime notifica
   ‚îî‚îÄ Canvas actualiza

9. Usuario presiona "Save Preset"
   ‚îî‚îÄ Dialog: "Studio Lighting Preset"
   ‚îî‚îÄ Selecciona locked_pillars: [LIGHTSCALER]
   ‚îî‚îÄ POST /api/presets/create
   ‚îî‚îÄ Backend:
       ‚îî‚îÄ Compila prompt (OCULTO)
       ‚îî‚îÄ Inserta en smart_presets {locked_pillars: [LIGHTSCALER]}
   ‚îî‚îÄ Respuesta: "Preset saved! Locked: LIGHTSCALER. Free to adjust: PHOTOSCALER, STYLESCALER"

10. Usuario descarga imagen
    ‚îî‚îÄ GET /storage/results/abc123.jpg
    ‚îî‚îÄ Guardado en m√°quina
```

---

## CONFIGURACI√ìN CLOUD: DEPLOYMENT DETALLADO

### OPCI√ìN RECOMENDADA: Arquitectura H√≠brida Google Cloud + Cloudflare

#### A. Cloudflare Workers (Edge Gateway)

**Workflow:**
```
Cloudflare Dashboard ‚Üí Workers KV Store ‚Üí Create Worker Script
```

**C√≥digo Deploy:**

```bash
# Step 1: Instala Wrangler CLI
npm install -g @cloudflare/wrangler

# Step 2: Inicializa proyecto
wrangler init luxscaler-edge

# Step 3: Copia worker code
cp backend/cloudflare-worker/api-gateway.ts ./src/index.ts

# Step 4: Configura wrangler.toml
```

```toml
# wrangler.toml
name = "luxscaler-edge"
type = "service"

[env.production]
routes = [{ pattern = "api.luxscaler.com/*", zone_id = "YOUR_ZONE_ID" }]

[build]
command = "npm run build"

[[env.production.kv_namespaces]]
binding = "CACHE"
id = "your_kv_namespace_id"

[[env.production.triggers.crons]]
cron = "0 * * * *"  # Cada hora
```

**Deploy:**
```bash
wrangler publish
```

#### B. Google Cloud Run (Backend Workers)

**Configuraci√≥n:**

```bash
# Step 1: Autentica con Google Cloud
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# Step 2: Crea Dockerfile para Cloud Run
cat > Dockerfile << EOF
FROM node:20-alpine

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .

EXPOSE 8080
CMD ["node", "dist/index.js"]
EOF

# Step 3: Construye imagen
docker build -t gcr.io/YOUR_PROJECT_ID/luxscaler-worker:latest .

# Step 4: Pushea a Google Container Registry
docker push gcr.io/YOUR_PROJECT_ID/luxscaler-worker:latest

# Step 5: Deploy a Cloud Run
gcloud run deploy luxscaler-worker \
  --image gcr.io/YOUR_PROJECT_ID/luxscaler-worker:latest \
  --platform managed \
  --region europe-west1 \
  --memory 4Gi \
  --cpu 4 \
  --timeout 3600 \
  --set-env-vars "SUPABASE_URL=YOUR_URL,SUPABASE_KEY=YOUR_KEY,GCP_PROJECT_ID=YOUR_PROJECT" \
  --allow-unauthenticated \
  --no-traffic  # Recibe tr√°fico via Cloud Tasks
```

#### C. Google Cloud Tasks (Job Queue)

**Configuraci√≥n:**

```bash
# Step 1: Crea cola
gcloud tasks queues create luxscaler-processing-queue \
  --location=europe-west1 \
  --max-concurrent-dispatches=100 \
  --max-dispatches-per-second=100

# Step 2: Configura retry policy
gcloud tasks queues update luxscaler-processing-queue \
  --location=europe-west1 \
  --max-attempts=3 \
  --min-backoff=5s \
  --max-backoff=1h
```

#### D. Supabase (Database + Realtime + Storage)

**Setup:**

```bash
# Step 1: Crea proyecto en supabase.com
# Step 2: Ejecuta schema SQL (from section "SCHEMA SQL COMPLETO v28.0")
# Step 3: Configura Storage buckets

# Bucket 1: processing-bucket (para input/output/presets)
gsutil mb gs://YOUR_PROJECT-processing-bucket

# Bucket 2: Supabase Storage (via dashboard)
#  - Create bucket: "processing-bucket"
#  - Set RLS policies
```

**Environment Variables:**

```bash
# .env.production
SUPABASE_URL="https://YOUR_PROJECT.supabase.co"
SUPABASE_KEY="eyJ..."
SUPABASE_SERVICE_ROLE_KEY="eyJ..."

# Google Cloud
GCP_PROJECT_ID="your-project-id"
GCP_REGION="europe-west1"
VERTEX_AI_API_KEY="ya29..."

# Cloudflare
CLOUDFLARE_API_TOKEN="YOUR_TOKEN"
CLOUDFLARE_ZONE_ID="YOUR_ZONE_ID"

# Cloud Tasks
CLOUD_TASKS_QUEUE="luxscaler-processing-queue"
```

#### E. CI/CD Pipeline (GitHub Actions)

```yaml
# .github/workflows/deploy.yml

name: Deploy LuxScaler v28.0

on:
  push:
    branches: [main]

jobs:
  deploy-cloudflare:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci && npm run build
      - uses: cloudflare/wrangler-action@v3
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}

  deploy-cloud-run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: google-github-actions/setup-gcloud@v1
        with:
          service_account_key: ${{ secrets.GCP_SA_KEY }}
          project_id: ${{ secrets.GCP_PROJECT_ID }}
      - run: |
          docker build -t gcr.io/${{ secrets.GCP_PROJECT_ID }}/luxscaler-worker:${{ github.sha }} .
          docker push gcr.io/${{ secrets.GCP_PROJECT_ID }}/luxscaler-worker:${{ github.sha }}
      - run: |
          gcloud run deploy luxscaler-worker \
            --image gcr.io/${{ secrets.GCP_PROJECT_ID }}/luxscaler-worker:${{ github.sha }} \
            --region europe-west1 \
            --update-env-vars GCP_PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}
```

---

## MONITOREO, M√âTRICAS Y OBSERVABILIDAD

### Google Cloud Monitoring

```typescript
// File: backend/services/monitoring.ts

import { MetricServiceClient } from "@google-cloud/monitoring";

interface CustomMetric {
  metric_type: string;
  value: number;
  timestamp: Date;
}

async function recordCustomMetric(metric: CustomMetric) {
  const client = new MetricServiceClient();

  const dataPoint = {
    interval: {
      endTime: { seconds: Math.floor(Date.now() / 1000) },
    },
    value: { doubleValue: metric.value },
  };

  const timeSeries = {
    metric: { type: `custom.googleapis.com/${metric.metric_type}` },
    resource: {
      type: "global",
      labels: { project_id: process.env.GCP_PROJECT_ID },
    },
    points: [dataPoint],
  };

  const request = {
    name: client.projectPath(process.env.GCP_PROJECT_ID),
    timeSeries: [timeSeries],
  };

  await client.createTimeSeries(request);
}

// Ejemplo de uso
await recordCustomMetric({
  metric_type: "luxscaler/job_processing_time_ms",
  value: executionTime,
  timestamp: new Date(),
});
```

### M√©tricas Clave

| M√©trica | Target | Alert Threshold |
|---|---|---|
| Job Processing Time (ms) | <5000 | >10000 |
| Context Cache Hit Rate (%) | >80% | <60% |
| Vision Analysis Cost per Job ($) | <0.05 | >0.10 |
| Gemini API Error Rate (%) | <1% | >5% |
| Cloud Run Memory Usage (%) | <70% | >90% |
| Job Completion Rate (%) | >99% | <95% |
| Supabase Database Size (GB) | <100 | >150 |

### Cloud Logging

```typescript
// File: backend/services/logging.ts

import { Logging } from "@google-cloud/logging";

const logging = new Logging({
  projectId: process.env.GCP_PROJECT_ID,
});

const log = logging.log("luxscaler-v28");

async function logJobEvent(jobId: string, event: any) {
  const entry = log.entry(
    {
      severity: "INFO",
      labels: { job_id: jobId },
    },
    event
  );

  await log.write(entry);
}

// Ejemplo
await logJobEvent("abc123", {
  event: "job_completed",
  execution_time_ms: 5432,
  output_url: "https://...",
  cache_hit: true,
});
```

---

## ROADMAP POST-V28.0

### v28.1 (Febrero 2026): Performance Tuning
- [ ] Implement advanced caching strategies
- [ ] Optimize Gemini prompts for lower token usage
- [ ] Add batch processing for bulk uploads

### v28.2 (Marzo 2026): AI Safety & Compliance
- [ ] Add content moderation filters
- [ ] Implement GDPR data retention policies
- [ ] Add audit logging for compliance

### v29.0 (Q2 2026): Mobile Native
- [ ] iOS app with native Camera integration
- [ ] Android app with Google Pixel camera API
- [ ] Mobile-optimized preview rendering

### v29.1 (Q2 2026): Collaborative Features
- [ ] Real-time preset sharing & collaboration
- [ ] Team workspaces (PROLUX)
- [ ] Preset remixing and versioning

### v30.0 (Q3 2026): Advanced AI
- [ ] Fine-tuned Gemini models per use case
- [ ] Custom model training pipeline
- [ ] Federated learning for privacy-preserving improvements

---

## CONCLUSI√ìN

LuxScaler v28.0 representa una evoluci√≥n arquitect√≥nica completa de la plataforma de procesamiento fotogr√°fico:

**Logros principales:**
- ‚úÖ Latencia percibida cero mediante arquitectura as√≠ncrona
- ‚úÖ 60% reducci√≥n en costos de API mediante Context Caching y Proxy Vision
- ‚úÖ 95% precisi√≥n en m√°scaras mediante Code Execution
- ‚úÖ Identidad biom√©trica garantizada mediante Multimodal DNA Anchor
- ‚úÖ Presets opacos y seguros con Smart Lock
- ‚úÖ Monetizaci√≥n clara y escalable por perfil
- ‚úÖ Observabilidad completa con Google Cloud + Supabase

**Stack final:**
- Frontend: React 19 + Realtime Supabase
- Edge: Cloudflare Workers (latencia <100ms)
- Backend: Google Cloud Run (timeout 60min)
- Queue: Google Cloud Tasks
- Database: Supabase + pgvector
- AI: Vertex AI (Gemini 3 Pro + Context Caching + Code Execution)

Esta arquitectura escala a millones de usuarios sin degradaci√≥n. üöÄ
```
